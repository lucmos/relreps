{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c313518f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "\n",
    "DEVICE: str = \"cpu\"\n",
    "NUM_ANCHORS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12cb026",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def relative_projection(x: torch.Tensor, anchors: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the relative representation of x with the cosine similarity\n",
    "\n",
    "    Args:\n",
    "        x: the samples absolute latents [batch, hidden_dim]\n",
    "        anchors: the anchors absolute latents [anchors, hidden_dim]\n",
    "\n",
    "    Returns:\n",
    "        the relative representation of x. The relative representation is *not* normalized,\n",
    "        when training on relative representation it is useful to normalize it\n",
    "    \"\"\"\n",
    "    x = F.normalize(x, p=2, dim=-1)\n",
    "    anchors = F.normalize(anchors, p=2, dim=-1)\n",
    "    return torch.einsum(\"bm, am -> ba\", x, anchors)\n",
    "\n",
    "\n",
    "class LatentSpace:\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding_type: str,\n",
    "        encoder_name: str,\n",
    "        vectors: torch.Tensor,\n",
    "        ids: Sequence[int],\n",
    "    ):\n",
    "        \"\"\"Utility class to represent a generic latent space\n",
    "\n",
    "        Args:\n",
    "            encoding_type: the type of latent space, i.e. \"absolute\" or \"relative\" usually\n",
    "            encoder_name: the name of the encoder used to obtain the vectors\n",
    "            vectors: the latents that compose the latent space\n",
    "            ids: the ids associated ot the vectors\n",
    "        \"\"\"\n",
    "        assert vectors.shape[0] == len(ids)\n",
    "\n",
    "        self.encoding_type: str = encoding_type\n",
    "        self.vectors: torch.Tensor = vectors\n",
    "        self.ids: Sequence[int] = ids\n",
    "        self.encoder_name: str = encoder_name\n",
    "\n",
    "    def get_anchors(self, anchor_choice: str, num_anchors: int, seed: int) -> Sequence[int]:\n",
    "        \"\"\"Adopt some strategy to select the anchors.\n",
    "\n",
    "        Args:\n",
    "            anchor_choice: the selection strategy for the anchors\n",
    "            seed: the random seed to use\n",
    "\n",
    "        Returns:\n",
    "            the ids of the chosen anchors\n",
    "        \"\"\"\n",
    "        # Select anchors\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if anchor_choice == \"uniform\":\n",
    "            limit: int = len(self.ids) if anchor_choice == \"uniform\" else int(anchor_choice[4:])\n",
    "            anchor_set: Sequence[int] = random.sample(self.ids[:limit], num_anchors)\n",
    "        else:\n",
    "            assert NotImplementedError\n",
    "\n",
    "        result = sorted(anchor_set)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def to_relative(\n",
    "        self, anchor_choice: str = None, seed: int = None, anchors: Optional[Sequence[int]] = None\n",
    "    ) -> \"RelativeSpace\":\n",
    "        \"\"\"Compute the relative transformation on the current space returning a new one.\n",
    "\n",
    "        Args:\n",
    "            anchor_choice: the anchors selection strategy to use, if no anchors are provided\n",
    "            seed: the random seed to use\n",
    "            anchors: the ids of the anchors to use\n",
    "\n",
    "        Returns:\n",
    "            the RelativeSpace associated to the current LatentSpace\n",
    "        \"\"\"\n",
    "        assert self.encoding_type != \"relative\"  # TODO: for now\n",
    "        anchors = self.get_anchors(anchor_choice=anchor_choice, seed=seed) if anchors is None else anchors\n",
    "\n",
    "        anchor_latents: torch.Tensor = self.vectors[anchors]\n",
    "\n",
    "        relative_vectors = relative_projection(x=self.vectors, anchors=anchor_latents.cpu())\n",
    "\n",
    "        return RelativeSpace(vectors=relative_vectors, encoder_name=self.encoder_name, anchors=anchors, ids=self.ids)\n",
    "\n",
    "\n",
    "class RelativeSpace(LatentSpace):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vectors: torch.Tensor,\n",
    "        ids: Sequence[int],\n",
    "        anchors: Sequence[int],\n",
    "        encoder_name: str = None,\n",
    "    ):\n",
    "        \"\"\"Utility class to represent a relative latent space\n",
    "\n",
    "        Args:\n",
    "            vectors: the latents that compose the latent space\n",
    "            ids: the ids associated ot the vectors\n",
    "            encoder_name: the name of the encoder_name used to obtain the vectors\n",
    "            anchors: the ids associated to the anchors to use\n",
    "        \"\"\"\n",
    "        super().__init__(encoding_type=\"relative\", vectors=vectors, encoder_name=encoder_name, ids=ids)\n",
    "        self.anchors: Sequence[int] = anchors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0b2152",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 128]) torch.Size([1000, 128])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0180</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.0180\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import cosine_similarity\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# Some fake absolute latents\n",
    "absolute_latents = torch.randn((NUM_SAMPLES, HIDDEN_DIM))\n",
    "\n",
    "# Apply a perfect isometry to the fake absolute latents\n",
    "isometric_transformation = torch.tensor(ortho_group.rvs(HIDDEN_DIM), dtype=torch.float)\n",
    "isometric_absolute_latents = absolute_latents @ isometric_transformation\n",
    "\n",
    "\n",
    "latent_space = LatentSpace(\n",
    "    encoding_type=\"absolute\",\n",
    "    encoder_name=\"random_vectors\",\n",
    "    vectors=absolute_latents,\n",
    "    ids=list(range(NUM_SAMPLES)),\n",
    ")\n",
    "iso_latent_space = LatentSpace(\n",
    "    encoding_type=\"absolute\",\n",
    "    encoder_name=\"iso_random_vectors\",\n",
    "    vectors=isometric_absolute_latents,\n",
    "    ids=list(range(NUM_SAMPLES)),\n",
    ")\n",
    "\n",
    "# The shape is [num_samples, hidden_dim]\n",
    "print(latent_space.vectors.shape, iso_latent_space.vectors.shape)\n",
    "\n",
    "# Compare the absolute latents --> low similarity since there is an isometry\n",
    "cosine_similarity(latent_space.vectors, iso_latent_space.vectors).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ed4e91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 300]) torch.Size([1000, 300])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get some anchors\n",
    "anchors_ids = latent_space.get_anchors(anchor_choice=\"uniform\", num_anchors=NUM_ANCHORS, seed=0)\n",
    "\n",
    "# Transform both spaces w.r.t. the same anchors\n",
    "rel_latent_space = latent_space.to_relative(anchors=anchors_ids)\n",
    "rel_iso_latent_space = iso_latent_space.to_relative(anchors=anchors_ids)\n",
    "\n",
    "# The shape is [num_samples, num_anchors]\n",
    "print(rel_latent_space.vectors.shape, rel_iso_latent_space.vectors.shape)\n",
    "\n",
    "# Compare the relative spaces --> perfect similarity, since we are invariant to isometries\n",
    "cosine_similarity(rel_latent_space.vectors, rel_iso_latent_space.vectors).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adf15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6fd7516f2b05119b428331a49d47507901bc91bf66ac5fe84be65c199d11674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
