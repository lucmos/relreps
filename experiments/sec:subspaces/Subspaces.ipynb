{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rae import PROJECT_ROOT\n",
    "from rae.modules.attention import RelativeAttention, AttentionOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf19995",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir: Path = PROJECT_ROOT / \"data\" / \"hf_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbf2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(str(datasets_dir / \"N24News\" / \"encoded\"))\n",
    "dataset.set_format(type=\"torch\", columns=[\"body_roberta-base\", \"image_vit_base_patch16_224\"], output_all_columns=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key: str = \"label\"\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbed74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rae.data.text.datamodule import AnchorsMode\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from typing import *\n",
    "\n",
    "\n",
    "def get_anchors(dataset, anchors_mode, anchors_num) -> Dict[str, Any]:\n",
    "    dataset_to_consider = dataset\n",
    "\n",
    "    if anchors_mode == AnchorsMode.DATASET:\n",
    "        return {\n",
    "            \"anchor_idxs\": list(range(len(dataset_to_consider))),\n",
    "            \"anchor_samples\": list(dataset_to_consider),\n",
    "            \"anchor_targets\": dataset_to_consider[target_key],\n",
    "            \"anchor_classes\": dataset_to_consider.classes,\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.STRATIFIED_SUBSET:\n",
    "        shuffled_idxs, shuffled_targets = shuffle(\n",
    "            np.asarray(list(range(len(dataset_to_consider)))),\n",
    "            np.asarray(dataset_to_consider[target_key]),\n",
    "            random_state=0,\n",
    "        )\n",
    "        all_targets = sorted(set(shuffled_targets))\n",
    "        class2idxs = {target: shuffled_idxs[shuffled_targets == target] for target in all_targets}\n",
    "\n",
    "        anchor_indices = []\n",
    "        i = 0\n",
    "        while len(anchor_indices) < anchors_num:\n",
    "            for target, target_idxs in class2idxs.items():\n",
    "                if i < len(target_idxs):\n",
    "                    anchor_indices.append(target_idxs[i])\n",
    "                if len(anchor_indices) == anchors_num:\n",
    "                    break\n",
    "            i += 1\n",
    "\n",
    "        anchors = [dataset_to_consider[int(idx)] for idx in anchor_indices]\n",
    "\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_indices,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.STRATIFIED:\n",
    "        if anchors_num >= len(dataset_to_consider.classes):\n",
    "            _, anchor_indices = train_test_split(\n",
    "                list(range(len(dataset_to_consider))),\n",
    "                test_size=anchors_num,\n",
    "                stratify=dataset_to_consider[target_key] if anchors_num >= len(dataset_to_consider.classes) else None,\n",
    "                random_state=0,\n",
    "            )\n",
    "        else:\n",
    "            assert False\n",
    "        anchors = [dataset_to_consider[int(idx)] for idx in anchor_indices]\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_indices,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.RANDOM_SAMPLES:\n",
    "        anchor_idxs = list(range(len(dataset_to_consider)))\n",
    "        random.shuffle(anchor_idxs)\n",
    "        anchors = [dataset_to_consider[index] for index in anchor_idxs]\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_idxs,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.RANDOM_LATENTS:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "\n",
    "anchors_num: int = 768\n",
    "anchor_idxs = get_anchors(train_dataset, anchors_mode=AnchorsMode.STRATIFIED_SUBSET, anchors_num=anchors_num)[\n",
    "    \"anchor_idxs\"\n",
    "]\n",
    "anchor_idxs = [int(x) for x in anchor_idxs]\n",
    "anchors = [train_dataset[anchor_idx] for anchor_idx in anchor_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17068e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_projection = RelativeAttention(\n",
    "    n_anchors=anchors_num,\n",
    "    normalization_mode=None,\n",
    "    similarity_mode=\"inner\",\n",
    "    values_mode=\"similarities\",\n",
    "    n_classes=train_dataset.features[target_key].num_classes,\n",
    "    output_normalization_mode=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ea17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# def fit(X, y):\n",
    "#     classifier = make_pipeline(\n",
    "#         Normalizer(), StandardScaler(), SVC(gamma=\"auto\", kernel=\"linear\", random_state=42)\n",
    "#     )  # , class_weight=\"balanced\"))\n",
    "#     classifier.fit(X, y)\n",
    "#     return lambda x: classifier.predict(x)\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Aggregate(nn.Module):\n",
    "    def __init__(self, num_signals: int, num_dims: int):\n",
    "        super().__init__()\n",
    "        self.num_signals: int = num_signals\n",
    "        self.num_dims = num_dims\n",
    "        # self.projection = nn.Linear(in_features=num_signals * num_dims, out_features=num_dims)\n",
    "        # self.weights = nn.Parameter(torch.ones(num_dims, num_signals))\n",
    "        # self.layer_norms = nn.ModuleList([\n",
    "        #     nn.LayerNorm(normalized_shape=anchors_num),\n",
    "        #     nn.LayerNorm(normalized_shape=anchors_num)\n",
    "        # ])\n",
    "        self.projection = nn.Linear(in_features=num_signals, out_features=1)\n",
    "        self.projections = nn.ModuleList([nn.Linear(num_dims, num_dims), nn.Linear(num_dims, num_dims)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.num_signals == 1:\n",
    "        #     return x\n",
    "        # x = torch.stack(x, dim=-1)\n",
    "        # return self.projection(x).squeeze(-1)\n",
    "        # return x.sum(dim=-1)\n",
    "        # if self.num_signals == 1:\n",
    "        #     return x.squeeze(-1)\n",
    "        #\n",
    "        x = torch.stack([self.projections[signal](x[..., signal]) for signal in range(x.size(-1))], dim=-1)\n",
    "        return self.projection(x).squeeze(-1)\n",
    "        # return torch.einsum(\"bes,esx->bex\", x, self.weights)   # TODO: we could use a mean or another aggregation mode\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     return self.projection(x.view(x.size(0), -1))\n",
    "\n",
    "\n",
    "def fit(X: torch.Tensor, y, seed):\n",
    "    seed_everything(seed)\n",
    "    dataset = TensorDataset(X, torch.as_tensor(y))\n",
    "    loader = DataLoader(dataset, batch_size=32, pin_memory=True, shuffle=True, num_workers=4)\n",
    "\n",
    "    model = [\n",
    "        Aggregate(num_signals=X.size(-1), num_dims=X.size(1)),\n",
    "        #\n",
    "        nn.LayerNorm(normalized_shape=anchors_num),\n",
    "        nn.Linear(in_features=anchors_num, out_features=anchors_num),\n",
    "        nn.SiLU(),\n",
    "        #\n",
    "        nn.BatchNorm1d(num_features=anchors_num),\n",
    "        nn.Linear(in_features=anchors_num, out_features=anchors_num),\n",
    "        nn.SiLU(),\n",
    "        #\n",
    "        nn.BatchNorm1d(num_features=anchors_num),\n",
    "        nn.Linear(in_features=anchors_num, out_features=train_dataset.features[target_key].num_classes),\n",
    "        nn.ReLU(),\n",
    "    ]\n",
    "\n",
    "    # if len(X.shape) == 3:\n",
    "    #     assert X.size(-1) > 1\n",
    "    #     model.insert(0, nn.Flatten(-2, -1))\n",
    "    #     model.insert(0, nn.Linear(in_features=X.size(-1), out_features=1))\n",
    "\n",
    "    model = nn.Sequential(*model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    opt = Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(5), leave=False, desc=\"epoch\"):\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = loss_fn(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "    model = model.eval().cpu()\n",
    "\n",
    "    return lambda x: model(x).argmax(-1).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import itertools\n",
    "\n",
    "NORMALIZE: bool = True\n",
    "train_y = train_dataset[target_key]\n",
    "test_y = test_dataset[target_key]\n",
    "anchor_latents = [anchor for anchor in anchors]\n",
    "\n",
    "result = defaultdict(list)\n",
    "for feature_names, embedding_type, seed in itertools.product(\n",
    "    ((\"body_roberta-base\", \"image_vit_base_patch16_224\"), (\"body_roberta-base\",), (\"image_vit_base_patch16_224\",)),\n",
    "    (\"absolute\", \"relative\"),\n",
    "    range(1),\n",
    "):\n",
    "    train_feature2latents = {feature_name: train_dataset[feature_name] for feature_name in feature_names}\n",
    "    test_feature2latents = {feature_name: test_dataset[feature_name] for feature_name in feature_names}\n",
    "\n",
    "    if NORMALIZE:\n",
    "        train_feature2latents = {\n",
    "            feature_name: F.normalize(latents, dim=-1, p=2) for feature_name, latents in train_feature2latents.items()\n",
    "        }\n",
    "        test_feature2latents = {\n",
    "            feature_name: F.normalize(latents, dim=-1, p=2) for feature_name, latents in test_feature2latents.items()\n",
    "        }\n",
    "\n",
    "    if embedding_type == \"relative\":\n",
    "        # !!! DO NOT SWAP TEST AND TRAIN LATENTS !!!\n",
    "        with torch.no_grad():\n",
    "            test_feature2latents = {\n",
    "                feature_name: relative_projection(x=latents, anchors=train_feature2latents[feature_name][anchor_idxs])[\n",
    "                    AttentionOutput.SIMILARITIES\n",
    "                ]\n",
    "                for feature_name, latents in test_feature2latents.items()\n",
    "            }\n",
    "            train_feature2latents = {\n",
    "                feature_name: relative_projection(x=latents, anchors=latents[anchor_idxs])[AttentionOutput.SIMILARITIES]\n",
    "                for feature_name, latents in train_feature2latents.items()\n",
    "            }\n",
    "        # !!! REALLY, DON'T !!!\n",
    "\n",
    "    train_latents = torch.stack(list(train_feature2latents.values()), dim=-1)\n",
    "    model = fit(X=train_latents, y=train_y, seed=seed)\n",
    "\n",
    "    test_latents = torch.stack(list(test_feature2latents.values()), dim=-1)\n",
    "    preds = model(test_latents)\n",
    "\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(test_y, preds, average=\"weighted\")\n",
    "    result[\"embed_type\"].append(embedding_type)\n",
    "    result[\"encodings\"].append(feature_names)\n",
    "    result[\"precision\"].append(precision)\n",
    "    result[\"recall\"].append(recall)\n",
    "    result[\"fscore\"].append(fscore)\n",
    "    result[\"seed\"].append(seed)\n",
    "\n",
    "    print(pd.DataFrame(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(result)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79e07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
