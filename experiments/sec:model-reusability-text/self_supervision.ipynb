{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff9ee4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6f7ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import sklearn.pipeline\n",
    "import torch\n",
    "from nn_core.serialization import load_model, NNCheckpointIO\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, PreTrainedModel, PreTrainedTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e9290",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rae.data.text import TREC\n",
    "from rae.modules.attention import RelativeAttention, AttentionOutput\n",
    "from rae.pl_modules.pl_text_classifier import LightningTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fb97b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_cfg(ckpt_path: Path):\n",
    "    cfg = NNCheckpointIO.load(path=ckpt_path)[\"cfg\"]\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1e84e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_latent_space(metadata, validation_stats_df, x_data: str, y_data: str):\n",
    "    color_discrete_map = {\n",
    "        class_name: color\n",
    "        for class_name, color in zip(metadata.class_to_idx, px.colors.qualitative.Plotly[: len(metadata.class_to_idx)])\n",
    "    }\n",
    "\n",
    "    latent_val_fig = px.scatter(\n",
    "        validation_stats_df,\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        category_orders={\"class_name\": metadata.class_to_idx.keys()},\n",
    "        #             # size='std_0',  # TODO: fixme, plotly crashes with any column name to set the anchor size\n",
    "        color=\"class_name\",\n",
    "        hover_name=\"image_index\",\n",
    "        hover_data=[\"image_index\", \"anchor_index\"],\n",
    "        facet_col=\"is_anchor\",\n",
    "        color_discrete_map=color_discrete_map,\n",
    "        # symbol=\"is_anchor\",\n",
    "        # symbol_map={False: \"circle\", True: \"star\"},\n",
    "        size_max=40,\n",
    "        # range_x=[-5, 5],\n",
    "        color_continuous_scale=None,\n",
    "        # range_y=[-5, 5],\n",
    "    )\n",
    "    return latent_val_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beedb177",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_ckpt(ckpt_path: Path):\n",
    "    return load_model(module_class=LightningTextClassifier, checkpoint_path=ckpt_path, strict=False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde0302",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CODE_VERSION = 0.1\n",
    "\n",
    "device: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e2eee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\"trec\")\n",
    "train_dataset = datasets[\"train\"]\n",
    "test_dataset = datasets[\"test\"]\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65296aa9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_key: str = \"label-coarse\"\n",
    "data_key: str = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02172ef1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class2idx = train_dataset.features[\"label-fine\"].str2int\n",
    "train_dataset.features[\"label-fine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb22c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_transformer(transformer_name):\n",
    "    transformer = AutoModel.from_pretrained(transformer_name, output_hidden_states=True, return_dict=True)\n",
    "    transformer.requires_grad_(False).eval()\n",
    "    return transformer, AutoTokenizer.from_pretrained(transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59419",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformer_names: str = [\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"google/electra-base-discriminator\",\n",
    "    \"roberta-base\",\n",
    "    # \"albert-base-v2\",\n",
    "    # \"distilbert-base-uncased\",\n",
    "    # \"distilbert-base-cased\",\n",
    "    \"xlm-roberta-base\",\n",
    "]\n",
    "\n",
    "transformers = {\n",
    "    transformer_name: load_transformer(transformer_name=transformer_name)\n",
    "    for transformer_name in transformer_names  # all these have latents already cached in latents.pt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280da62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f7213",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_y = np.array(train_dataset[target_key])\n",
    "test_y = np.array(test_dataset[target_key])\n",
    "len(set(train_y)), len(set(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15452adc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9d35e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def call_transformer(batch, transformer):\n",
    "    encoding = batch[\"encoding\"].to(device)\n",
    "    sample_encodings = transformer(**encoding)[\"hidden_states\"][-1]\n",
    "    # TODO: aggregation mode\n",
    "    result = []\n",
    "    for sample_encoding, sample_mask in zip(sample_encodings, batch[\"mask\"]):\n",
    "        result.append(sample_encoding[sample_mask].mean(dim=0))\n",
    "\n",
    "    return torch.stack(result, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd0fbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rae.data.text.datamodule import AnchorsMode\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42de90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "\n",
    "def get_anchors(dataset, anchors_mode, anchors_num) -> Dict[str, Any]:\n",
    "    dataset_to_consider = dataset\n",
    "\n",
    "    if anchors_mode == AnchorsMode.DATASET:\n",
    "        return {\n",
    "            \"anchor_idxs\": list(range(len(dataset_to_consider))),\n",
    "            \"anchor_samples\": list(dataset_to_consider),\n",
    "            \"anchor_targets\": dataset_to_consider[target_key],\n",
    "            \"anchor_classes\": dataset_to_consider.classes,\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.STRATIFIED_SUBSET:\n",
    "        shuffled_idxs, shuffled_targets = shuffle(\n",
    "            np.asarray(list(range(len(dataset_to_consider)))),\n",
    "            np.asarray(dataset_to_consider[target_key]),\n",
    "            random_state=0,\n",
    "        )\n",
    "        all_targets = sorted(set(shuffled_targets))\n",
    "        class2idxs = {target: shuffled_idxs[shuffled_targets == target] for target in all_targets}\n",
    "\n",
    "        anchor_indices = []\n",
    "        i = 0\n",
    "        while len(anchor_indices) < anchors_num:\n",
    "            for target, target_idxs in class2idxs.items():\n",
    "                if i < len(target_idxs):\n",
    "                    anchor_indices.append(target_idxs[i])\n",
    "                if len(anchor_indices) == anchors_num:\n",
    "                    break\n",
    "            i += 1\n",
    "\n",
    "        anchors = [dataset_to_consider[int(idx)] for idx in anchor_indices]\n",
    "\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_indices,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.STRATIFIED:\n",
    "        if anchors_num >= len(dataset_to_consider.classes):\n",
    "            _, anchor_indices = train_test_split(\n",
    "                list(range(len(dataset_to_consider))),\n",
    "                test_size=anchors_num,\n",
    "                stratify=dataset_to_consider[target_key] if anchors_num >= len(dataset_to_consider.classes) else None,\n",
    "                random_state=0,\n",
    "            )\n",
    "        else:\n",
    "            anchor_indices = HARDCODED_ANCHORS[:anchors_num]\n",
    "        anchors = [dataset_to_consider[int(idx)] for idx in anchor_indices]\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_indices,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.RANDOM_SAMPLES:\n",
    "        anchor_idxs = list(range(len(dataset_to_consider)))\n",
    "        random.shuffle(anchor_idxs)\n",
    "        anchors = [dataset_to_consider[index] for index in anchor_idxs]\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_idxs,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.RANDOM_LATENTS:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "\n",
    "anchors_num: int = 768\n",
    "anchor_idxs = get_anchors(train_dataset, anchors_mode=AnchorsMode.STRATIFIED_SUBSET, anchors_num=anchors_num)[\n",
    "    \"anchor_idxs\"\n",
    "]\n",
    "anchor_idxs = [int(x) for x in anchor_idxs]\n",
    "anchors = [train_dataset[anchor_idx] for anchor_idx in anchor_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8bc98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relative_projection = RelativeAttention(\n",
    "    n_anchors=anchors_num,\n",
    "    normalization_mode=\"l2\",\n",
    "    similarity_mode=\"inner\",\n",
    "    values_mode=\"similarities\",\n",
    "    n_classes=train_dataset.features[target_key].num_classes,\n",
    "    output_normalization_mode=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe4d83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    encoding = tokenizer(\n",
    "        [sample[data_key] for sample in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    mask = encoding[\"attention_mask\"] * encoding[\"special_tokens_mask\"].bool().logical_not()\n",
    "    del encoding[\"special_tokens_mask\"]\n",
    "    return {\"encoding\": encoding, \"mask\": mask.bool()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f43de5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a47936",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_latents(dataloader, anchors, split: str, transformer) -> Dict[str, torch.Tensor]:\n",
    "    absolute_latents: List = []\n",
    "    relative_latents: List = []\n",
    "\n",
    "    transformer = transformer.to(device)\n",
    "    for batch in tqdm(dataloader, desc=f\"[{split}] Computing latents\"):\n",
    "        with torch.no_grad():\n",
    "            batch_latents = call_transformer(batch=batch, transformer=transformer)\n",
    "\n",
    "            absolute_latents.append(batch_latents)\n",
    "\n",
    "            if anchors is not None:\n",
    "                batch_rel_latents = relative_projection.encode(x=batch_latents, anchors=anchors)[\n",
    "                    AttentionOutput.SIMILARITIES\n",
    "                ]\n",
    "                relative_latents.append(batch_rel_latents)\n",
    "\n",
    "    absolute_latents: torch.Tensor = torch.cat(absolute_latents, dim=0).cpu()\n",
    "    relative_latents: torch.Tensor = (\n",
    "        torch.cat(relative_latents, dim=0).cpu() if len(relative_latents) > 0 else relative_latents\n",
    "    )\n",
    "\n",
    "    transformer = transformer.cpu()\n",
    "    return {\n",
    "        \"absolute\": absolute_latents,\n",
    "        \"relative\": relative_latents,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af1d42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "latents = {\n",
    "    transformer_name: {\n",
    "        \"anchors_latents\": (\n",
    "            anchors_latents := get_latents(\n",
    "                dataloader=DataLoader(\n",
    "                    anchors,\n",
    "                    num_workers=8,\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=partial(collate_fn, tokenizer=transformers[transformer_name][1]),\n",
    "                    batch_size=32,\n",
    "                ),\n",
    "                split=f\"{transformer_name}, anchor\",\n",
    "                anchors=None,\n",
    "                transformer=transformers[transformer_name][0],\n",
    "            )[\"absolute\"]\n",
    "        ),\n",
    "        **{\n",
    "            str(dataset_split.split): get_latents(\n",
    "                dataloader=DataLoader(\n",
    "                    dataset_split,\n",
    "                    num_workers=8,\n",
    "                    pin_memory=True,\n",
    "                    collate_fn=partial(collate_fn, tokenizer=transformers[transformer_name][1]),\n",
    "                    batch_size=32,\n",
    "                ),\n",
    "                split=f\"{transformer_name}, {str(dataset_split.split)}\",\n",
    "                anchors=anchors_latents.to(device),\n",
    "                transformer=transformers[transformer_name][0],\n",
    "            )\n",
    "            for dataset_split in [train_dataset, test_dataset]\n",
    "        },\n",
    "    }\n",
    "    for transformer_name in transformers\n",
    "}\n",
    "latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e3436",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292d735",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# def fit(X, y):\n",
    "#     classifier = make_pipeline(\n",
    "#         Normalizer(), StandardScaler(), SVC(gamma=\"auto\", kernel=\"linear\", random_state=42)\n",
    "#     )  # , class_weight=\"balanced\"))\n",
    "#     classifier.fit(X, y)\n",
    "#     return lambda x: classifier.predict(x)\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "\n",
    "def fit(X, y, seed):\n",
    "    seed_everything(seed)\n",
    "    dataset = TensorDataset(X, torch.as_tensor(y))\n",
    "    loader = DataLoader(dataset, batch_size=32, pin_memory=True, shuffle=True, num_workers=4)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=anchors_num),\n",
    "        nn.Linear(in_features=anchors_num, out_features=anchors_num),\n",
    "        nn.SiLU(),\n",
    "        nn.BatchNorm1d(num_features=anchors_num),\n",
    "        nn.Linear(in_features=anchors_num, out_features=anchors_num),\n",
    "        nn.SiLU(),\n",
    "        nn.BatchNorm1d(num_features=anchors_num),\n",
    "        nn.Linear(in_features=anchors_num, out_features=train_dataset.features[target_key].num_classes),\n",
    "        nn.ReLU(),\n",
    "    ).to(device)\n",
    "    opt = Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(5), leave=False, desc=\"batch\"):\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred_y = model(batch_x)\n",
    "            loss = loss_fn(pred_y, batch_y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "    model = model.cpu()\n",
    "    return lambda x: model(x).argmax(-1).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951c369",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEEDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "\n",
    "fitted_classifiers = {\n",
    "    seed: {\n",
    "        transformer_name: {\n",
    "            embedding_type: fit(latents[transformer_name][\"train\"][embedding_type], train_y, seed)\n",
    "            for embedding_type in tqdm([\"absolute\", \"relative\"], leave=False, desc=\"embedding_type\")\n",
    "        }\n",
    "        for transformer_name in tqdm(transformers, desc=\"transformer\")\n",
    "    }\n",
    "    for seed in SEEDS\n",
    "}\n",
    "fitted_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca059a8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "numeric_results = {\n",
    "    \"seed\": [],\n",
    "    \"embed_type\": [],\n",
    "    \"embed_transformer\": [],\n",
    "    \"classifier_transformer\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"fscore\": [],\n",
    "    \"stitched\": [],\n",
    "}\n",
    "for seed in SEEDS:\n",
    "    for embed_type in [\"absolute\", \"relative\"]:\n",
    "        for embed_transformer in transformers:\n",
    "            for classifier_transformer in transformers:\n",
    "                test_latents = latents[embed_transformer][\"test\"][embed_type]\n",
    "                classifier = fitted_classifiers[seed][classifier_transformer][embed_type]\n",
    "                preds = classifier(test_latents)\n",
    "\n",
    "                precision, recall, fscore, _ = precision_recall_fscore_support(test_y, preds, average=\"weighted\")\n",
    "                numeric_results[\"embed_type\"].append(embed_type)\n",
    "                numeric_results[\"embed_transformer\"].append(embed_transformer)\n",
    "                numeric_results[\"classifier_transformer\"].append(classifier_transformer)\n",
    "                numeric_results[\"precision\"].append(precision)\n",
    "                numeric_results[\"recall\"].append(recall)\n",
    "                numeric_results[\"fscore\"].append(fscore)\n",
    "                numeric_results[\"stitched\"].append(embed_transformer != classifier_transformer)\n",
    "                numeric_results[\"seed\"].append(seed)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "df = pd.DataFrame(numeric_results)\n",
    "df.to_csv(\"nlp_stitching.csv\", sep=\"\\t\")\n",
    "\n",
    "df = df.groupby(\n",
    "    [\n",
    "        \"embed_type\",\n",
    "        \"stitched\",\n",
    "        \"embed_transformer\",\n",
    "        \"classifier_transformer\",\n",
    "    ]\n",
    ").agg([np.mean, \"count\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3e07f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130be91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
