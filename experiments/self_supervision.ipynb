{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba09876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import sklearn.pipeline\n",
    "import torch\n",
    "from nn_core.serialization import load_model, NNCheckpointIO\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, PreTrainedModel, PreTrainedTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rae.data.text import TREC\n",
    "from rae.modules.attention import RelativeAttention, AttentionOutput\n",
    "from rae.pl_modules.pl_text_classifier import LightningTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a369f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cfg(ckpt_path: Path):\n",
    "    cfg = NNCheckpointIO.load(path=ckpt_path)[\"cfg\"]\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(metadata, validation_stats_df, x_data: str, y_data: str):\n",
    "    color_discrete_map = {\n",
    "        class_name: color\n",
    "        for class_name, color in zip(metadata.class_to_idx, px.colors.qualitative.Plotly[: len(metadata.class_to_idx)])\n",
    "    }\n",
    "\n",
    "    latent_val_fig = px.scatter(\n",
    "        validation_stats_df,\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        category_orders={\"class_name\": metadata.class_to_idx.keys()},\n",
    "        #             # size='std_0',  # TODO: fixme, plotly crashes with any column name to set the anchor size\n",
    "        color=\"class_name\",\n",
    "        hover_name=\"image_index\",\n",
    "        hover_data=[\"image_index\", \"anchor_index\"],\n",
    "        facet_col=\"is_anchor\",\n",
    "        color_discrete_map=color_discrete_map,\n",
    "        # symbol=\"is_anchor\",\n",
    "        # symbol_map={False: \"circle\", True: \"star\"},\n",
    "        size_max=40,\n",
    "        # range_x=[-5, 5],\n",
    "        color_continuous_scale=None,\n",
    "        # range_y=[-5, 5],\n",
    "    )\n",
    "    return latent_val_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e546db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(ckpt_path: Path):\n",
    "    return load_model(module_class=LightningTextClassifier, checkpoint_path=ckpt_path, strict=False).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb82576",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_VERSION = 0.1\n",
    "\n",
    "device: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\"trec\")\n",
    "train_dataset = datasets[\"train\"]\n",
    "test_dataset = datasets[\"test\"]\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_key: str = \"label-coarse\"\n",
    "data_key: str = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b552b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2idx = train_dataset.features[\"label-fine\"].str2int\n",
    "train_dataset.features[\"label-fine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformer(transformer_name):\n",
    "    transformer = AutoModel.from_pretrained(transformer_name, output_hidden_states=True, return_dict=True)\n",
    "    transformer.requires_grad_(False).eval().to(device)\n",
    "    return transformer, AutoTokenizer.from_pretrained(transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_name: str = [\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"google/electra-base-discriminator\",\n",
    "    \"roberta-base\",\n",
    "    \"albert-base-v2\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"distilbert-base-cased\",\n",
    "    \"xlm-roberta-base\",\n",
    "][0]\n",
    "transformer, tokenizer = load_transformer(transformer_name=transformer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93981bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_dataset[target_key])\n",
    "test_y = np.array(test_dataset[target_key])\n",
    "len(set(train_y)), len(set(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_transformer(batch, transformer):\n",
    "    encoding = batch[\"encoding\"].to(device)\n",
    "    sample_encodings = transformer(**encoding)[\"hidden_states\"][-1]\n",
    "    # TODO: aggregation mode\n",
    "    result = []\n",
    "    for sample_encoding, sample_mask in zip(sample_encodings, batch[\"mask\"]):\n",
    "        result.append(sample_encoding[sample_mask].mean(dim=0))\n",
    "\n",
    "    return torch.stack(result, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rae.data.text.datamodule import AnchorsMode\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdec912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "\n",
    "def get_anchors(dataset, anchors_mode, anchors_num) -> Dict[str, Any]:\n",
    "    dataset_to_consider = dataset\n",
    "\n",
    "    if anchors_mode == AnchorsMode.DATASET:\n",
    "        return {\n",
    "            \"anchor_idxs\": list(range(len(dataset_to_consider))),\n",
    "            \"anchor_samples\": list(dataset_to_consider),\n",
    "            \"anchor_targets\": dataset_to_consider[target_key],\n",
    "            \"anchor_classes\": dataset_to_consider.classes,\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.STRATIFIED_SUBSET:\n",
    "        shuffled_idxs, shuffled_targets = shuffle(\n",
    "            np.asarray(list(range(len(dataset_to_consider)))),\n",
    "            np.asarray(dataset_to_consider[target_key]),\n",
    "            random_state=0,\n",
    "        )\n",
    "        all_targets = sorted(set(shuffled_targets))\n",
    "        class2idxs = {target: shuffled_idxs[shuffled_targets == target] for target in all_targets}\n",
    "\n",
    "        anchor_indices = []\n",
    "        i = 0\n",
    "        while len(anchor_indices) < anchors_num:\n",
    "            for target, target_idxs in class2idxs.items():\n",
    "                if i < len(target_idxs):\n",
    "                    anchor_indices.append(target_idxs[i])\n",
    "                if len(anchor_indices) == anchors_num:\n",
    "                    break\n",
    "            i += 1\n",
    "\n",
    "        anchors = [dataset_to_consider[int(idx)] for idx in anchor_indices]\n",
    "\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_indices,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.STRATIFIED:\n",
    "        if anchors_num >= len(dataset_to_consider.classes):\n",
    "            _, anchor_indices = train_test_split(\n",
    "                list(range(len(dataset_to_consider))),\n",
    "                test_size=anchors_num,\n",
    "                stratify=dataset_to_consider[target_key] if anchors_num >= len(dataset_to_consider.classes) else None,\n",
    "                random_state=0,\n",
    "            )\n",
    "        else:\n",
    "            anchor_indices = HARDCODED_ANCHORS[:anchors_num]\n",
    "        anchors = [dataset_to_consider[int(idx)] for idx in anchor_indices]\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_indices,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.RANDOM_SAMPLES:\n",
    "        anchor_idxs = list(range(len(dataset_to_consider)))\n",
    "        random.shuffle(anchor_idxs)\n",
    "        anchors = [dataset_to_consider[index] for index in anchor_idxs]\n",
    "        return {\n",
    "            \"anchor_idxs\": anchor_idxs,\n",
    "            \"anchor_samples\": anchors,\n",
    "            \"anchor_targets\": [anchor[target_key] for anchor in anchors],\n",
    "            \"anchor_classes\": [\n",
    "                dataset_to_consider.features[target_key].int2str(anchor[target_key]) for anchor in anchors\n",
    "            ],\n",
    "            \"anchor_latents\": None,\n",
    "        }\n",
    "    elif anchors_mode == AnchorsMode.RANDOM_LATENTS:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise RuntimeError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_num: int = 768\n",
    "relative_projection = RelativeAttention(\n",
    "    n_anchors=anchors_num,\n",
    "    normalization_mode=\"l2\",\n",
    "    similarity_mode=\"inner\",\n",
    "    values_mode=\"similarities\",\n",
    "    n_classes=train_dataset.features[target_key].num_classes,\n",
    "    output_normalization_mode=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    encoding = tokenizer(\n",
    "        [sample[data_key] for sample in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    mask = encoding[\"attention_mask\"] * encoding[\"special_tokens_mask\"].bool().logical_not()\n",
    "    del encoding[\"special_tokens_mask\"]\n",
    "    return {\"encoding\": encoding, \"mask\": mask.bool()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_idxs = get_anchors(train_dataset, anchors_mode=AnchorsMode.STRATIFIED_SUBSET, anchors_num=anchors_num)[\n",
    "    \"anchor_idxs\"\n",
    "]\n",
    "anchor_idxs = [int(x) for x in anchor_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(dataloader, anchors, split: str, transformer) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    absolute_latents: List = []\n",
    "    relative_latents: List = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"[{split}] Computing relative latents\"):\n",
    "        with torch.no_grad():\n",
    "            batch_latents = call_transformer(batch=batch, transformer=transformer)\n",
    "            #             batch_latents = batch_latents[:, :anchors_num]\n",
    "\n",
    "            absolute_latents.append(batch_latents.cpu())\n",
    "\n",
    "            if anchors is not None:\n",
    "                batch_latents = relative_projection.encode(x=batch_latents, anchors=anchors)[\n",
    "                    AttentionOutput.SIMILARITIES\n",
    "                ].cpu()\n",
    "                relative_latents.append(batch_latents)\n",
    "\n",
    "    absolute_latents: torch.Tensor = torch.cat(absolute_latents, dim=0)\n",
    "    relative_latents: torch.Tensor = (\n",
    "        torch.cat(relative_latents, dim=0) if len(relative_latents) > 0 else relative_latents\n",
    "    )\n",
    "\n",
    "    return absolute_latents, relative_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6dddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "anchors = [train_dataset[anchor_idx] for anchor_idx in anchor_idxs]\n",
    "anchors, _ = get_latents(\n",
    "    dataloader=DataLoader(anchors, num_workers=8, pin_memory=True, collate_fn=collate_fn, batch_size=32),\n",
    "    split=\"anchor\",\n",
    "    anchors=None,\n",
    "    transformer=transformer,\n",
    ")\n",
    "anchors = anchors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_train_X, relative_train_X = get_latents(\n",
    "    dataloader=DataLoader(train_dataset, num_workers=8, pin_memory=True, collate_fn=collate_fn, batch_size=32),\n",
    "    split=\"train\",\n",
    "    anchors=anchors,\n",
    "    transformer=transformer,\n",
    ")\n",
    "absolute_train_X.shape, relative_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_test_X, relative_test_X = get_latents(\n",
    "    dataloader=DataLoader(test_dataset, num_workers=16, pin_memory=True, collate_fn=collate_fn, batch_size=64),\n",
    "    split=\"test\",\n",
    "    anchors=anchors,\n",
    "    transformer=transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(X, y):\n",
    "    classifier = make_pipeline(\n",
    "        Normalizer(), StandardScaler(), SVC(gamma=\"auto\", kernel=\"linear\", random_state=42)\n",
    "    )  # , class_weight=\"balanced\"))\n",
    "    return classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = test_dataset.features[target_key].names\n",
    "len(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute_classifier: sklearn.pipeline.Pipeline = svm_fit(absolute_train_X, train_y)\n",
    "# relative_classifier: sklearn.pipeline.Pipeline = svm_fit(relative_train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a209630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rae import PROJECT_ROOT\n",
    "\n",
    "absolute_classifier, relative_classifier = torch.load(PROJECT_ROOT / \"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_y_pred = absolute_classifier.predict(absolute_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a82e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, absolute_y_pred, target_names=target_names, output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_y_pred = relative_classifier.predict(relative_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, relative_y_pred, output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save((absolute_classifier, relative_classifier), PROJECT_ROOT / \"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3e07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
