{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1dee1ca4",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f3278c18",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "import logging\n",
            "\n",
            "import pandas as pd\n",
            "import torch\n",
            "import torch.nn.functional as F\n",
            "from rae.modules.enumerations import Output\n",
            "from rae.pl_modules.pl_gautoencoder import LightningAutoencoder\n",
            "\n",
            "try:\n",
            "    # be ready for 3.10 when it drops\n",
            "    from enum import StrEnum\n",
            "except ImportError:\n",
            "    from backports.strenum import StrEnum\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "from tueplots import bundles\n",
            "from tueplots import figsizes\n",
            "\n",
            "logging.getLogger().setLevel(logging.ERROR)\n",
            "\n",
            "DEVICE: str = \"cpu\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "99d998ac",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from rae.modules.text.encoder import GensimEncoder\n",
            "\n",
            "ENCODERS = {\n",
            "    model_name: GensimEncoder(language=\"en\", lemmatize=False, model_name=model_name)\n",
            "    for model_name in (\n",
            "        \"local_fasttext\",\n",
            "        \"word2vec-google-news-300\",\n",
            "    )\n",
            "}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d13cd066",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "assert len({frozenset(encoder.model.key_to_index.keys()) for encoder in ENCODERS.values()}) == 1"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2e385241",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "import random\n",
            "from pytorch_lightning import seed_everything\n",
            "\n",
            "seed_everything(4)\n",
            "\n",
            "NUM_ANCHORS = 300\n",
            "NUM_TARGETS = 200\n",
            "NUM_CLUSTERS = 3\n",
            "WORDS = sorted(ENCODERS[\"local_fasttext\"].model.key_to_index.keys())\n",
            "WORDS = [word for word in WORDS if word.isalpha() and len(word) >= 4]\n",
            "TARGET_WORDS = [\"school\", \"ferrari\", \"water\", \"martial\"]  # words to take the neighborhoods from\n",
            "# TARGET_WORDS = random.sample(WORDS, NUM_CLUSTERS)\n",
            "print(f\"{TARGET_WORDS=}\")\n",
            "word2index = {word: i for i, word in enumerate(WORDS)}\n",
            "TARGETS = torch.zeros(len(WORDS), device=\"cpu\")\n",
            "target_cluster = [\n",
            "    [word for word, sim in ENCODERS[\"local_fasttext\"].model.most_similar(target_word, topn=NUM_TARGETS)]\n",
            "    for target_word in TARGET_WORDS\n",
            "]\n",
            "\n",
            "valid_words, valid_targets = [], []\n",
            "for i, target_cluster in enumerate(target_cluster):\n",
            "    valid_words.append(TARGET_WORDS[i])\n",
            "    valid_targets.append(i + 1)\n",
            "    for word in target_cluster:\n",
            "        if word in word2index:\n",
            "            valid_words.append(word)\n",
            "            valid_targets.append(i + 1)\n",
            "\n",
            "WORDS = valid_words\n",
            "TARGETS = valid_targets\n",
            "\n",
            "ANCHOR_WORDS = sorted(random.sample(WORDS, NUM_ANCHORS))  # TODO: stratified\n",
            "\n",
            "ANCHOR_WORDS[:10]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "148843f7",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from sklearn.decomposition import PCA\n",
            "\n",
            "\n",
            "def latents_distance(latents):\n",
            "    assert len(latents) == 2\n",
            "    for x in latents:\n",
            "        assert x.shape[1] == 300\n",
            "\n",
            "    dist = F.pairwise_distance(latents[0], latents[1], p=2).mean()\n",
            "    return f\"{dist:.2f}\"\n",
            "\n",
            "\n",
            "def get_latents(words, encoder: GensimEncoder):\n",
            "    latents = torch.tensor([encoder.model.get_vector(word) for word in words], device=DEVICE)\n",
            "    return latents\n",
            "\n",
            "\n",
            "def to_df(latents, fit_pca: bool = True):\n",
            "    if fit_pca:\n",
            "        latents2d = PCA(n_components=2).fit_transform(latents.cpu())\n",
            "    else:\n",
            "        latents2d = latents[:, [0, 1]]\n",
            "    df = pd.DataFrame(\n",
            "        {\n",
            "            \"x\": latents2d[:, 0].tolist(),\n",
            "            \"y\": latents2d[:, 1].tolist(),\n",
            "            \"target\": TARGETS,\n",
            "        }\n",
            "    )\n",
            "    return df"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ed0388f8",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "# Plot stuff"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "61c6b02f",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "def plot_bg(\n",
            "    ax,\n",
            "    df,\n",
            "    cmap,\n",
            "    norm,\n",
            "    size=0.5,\n",
            "    bg_alpha=0.01,\n",
            "):\n",
            "    \"\"\"Create and return a plot of all our movie embeddings with very low opacity.\n",
            "    (Intended to be used as a basis for further - more prominent - plotting of a\n",
            "    subset of movies. Having the overall shape of the map space in the background is\n",
            "    useful for context.)\n",
            "    \"\"\"\n",
            "    ax.scatter(df.x, df.y, c=cmap(norm(df[\"target\"])), alpha=bg_alpha, s=size)\n",
            "    return ax\n",
            "\n",
            "\n",
            "def hightlight_cluster(\n",
            "    ax,\n",
            "    df,\n",
            "    target,\n",
            "    alpha,\n",
            "    cmap,\n",
            "    norm,\n",
            "    size=0.5,\n",
            "):\n",
            "    cluster_df = df[df[\"target\"] == target]\n",
            "    ax.scatter(cluster_df.x, cluster_df.y, c=cmap(norm(cluster_df[\"target\"])), alpha=alpha, s=size)\n",
            "\n",
            "\n",
            "def plot_latent_space(ax, df, targets, size, cmap, norm, bg_alpha=0.1, alpha=0.5):\n",
            "    ax = plot_bg(ax, df, bg_alpha=bg_alpha, cmap=cmap, norm=norm)\n",
            "    for target in targets:\n",
            "        hightlight_cluster(ax, df, target, alpha=alpha, size=size, cmap=cmap, norm=norm)\n",
            "    return ax"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "13da9076",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "## AE"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c47ff8cd",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "ae_latents = {}\n",
            "anchors_latents = {}\n",
            "for enc_name, encoder in ENCODERS.items():\n",
            "    ae_latents[enc_name] = get_latents(words=WORDS, encoder=encoder)\n",
            "    anchors_latents[enc_name] = get_latents(words=ANCHOR_WORDS, encoder=encoder)\n",
            "\n",
            "import copy\n",
            "\n",
            "original_ae_latents = copy.deepcopy(ae_latents)\n",
            "original_anchor_latents = copy.deepcopy(anchors_latents)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e466ea65",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "## Rel Attention Quantized"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fdf7c14e",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from sklearn.decomposition import PCA\n",
            "from rae.modules.attention import *\n",
            "\n",
            "\n",
            "col_config = (\n",
            "    (None, None),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.0001),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.05),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.1),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.3),\n",
            "    (\"cluster\", 1),\n",
            "    (\"cluster\", 1.5),\n",
            "    (\"cluster\", 2),\n",
            "    # ('kmeans', 3),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.6),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.7),\n",
            "    #     (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.8),\n",
            "    #    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.9),\n",
            ")\n",
            "N_ROWS = len(ENCODERS)\n",
            "N_COLS = len(col_config) + 1\n",
            "\n",
            "plt.rcParams.update(bundles.icml2022())\n",
            "plt.rcParams.update(figsizes.icml2022_full(ncols=N_COLS, nrows=N_ROWS, height_to_width_ratio=1.0))\n",
            "\n",
            "import matplotlib as mpl\n",
            "\n",
            "num_colors = len(TARGET_WORDS)\n",
            "cmap = mpl.colors.ListedColormap(plt.cm.get_cmap(\"Set1\", 10).colors[:num_colors], name=\"rgb\", N=num_colors)\n",
            "# cmap = plt.cm.get_cmap(\"Set1\", 5)\n",
            "norm = plt.Normalize(min(TARGETS), max(TARGETS))\n",
            "\n",
            "fig, axes = plt.subplots(dpi=150, nrows=N_ROWS, ncols=N_COLS, sharey=True, sharex=True, squeeze=True)\n",
            "\n",
            "\n",
            "TARGETS_HIGHTLIGHT = [1]\n",
            "for ax_encoders, (_, latents) in zip(axes, ae_latents.items()):\n",
            "\n",
            "    plot_latent_space(\n",
            "        ax_encoders[0],\n",
            "        to_df(latents),\n",
            "        targets=TARGETS_HIGHTLIGHT,\n",
            "        size=0.75,\n",
            "        bg_alpha=0.25,\n",
            "        alpha=1,\n",
            "        cmap=cmap,\n",
            "        norm=norm,\n",
            "    )\n",
            "\n",
            "distances = {\"absolute\": latents_distance(list(ae_latents.values()))}\n",
            "\n",
            "for col_i, (quant_mode, bin_size) in enumerate(col_config):\n",
            "    rel_attention = RelativeAttention(\n",
            "        n_anchors=NUM_ANCHORS,\n",
            "        n_classes=len(set(TARGETS)),\n",
            "        similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "        values_mode=ValuesMethod.SIMILARITIES,\n",
            "        normalization_mode=NormalizationMode.L2,\n",
            "        #  output_normalization_mode=OutputNormalization.L2,\n",
            "        #  similarities_quantization_mode=quant_mode,\n",
            "        #  similarities_bin_size=bin_size,\n",
            "        #  similarities_num_clusters=bin_size,\n",
            "        absolute_quantization_mode=quant_mode,\n",
            "        absolute_bin_size=bin_size,\n",
            "        absolute_num_clusters=bin_size,\n",
            "    )\n",
            "    assert sum(x.numel() for x in rel_attention.parameters()) == 0\n",
            "    rels = []\n",
            "    for row_axes, (enc_name, latents), (a_enc_name, a_latents) in zip(\n",
            "        axes, ae_latents.items(), anchors_latents.items()\n",
            "    ):\n",
            "        assert enc_name == a_enc_name\n",
            "        rel = rel_attention(x=latents, anchors=a_latents)[AttentionOutput.SIMILARITIES]\n",
            "        rels.append(rel)\n",
            "        plot_latent_space(\n",
            "            row_axes[col_i + 1],\n",
            "            to_df(rel),\n",
            "            targets=TARGETS_HIGHTLIGHT,\n",
            "            size=0.75,\n",
            "            bg_alpha=0.25,\n",
            "            alpha=1,\n",
            "            cmap=cmap,\n",
            "            norm=norm,\n",
            "        )\n",
            "    distances[f\"relative({quant_mode}, {bin_size})\"] = latents_distance(rels)\n",
            "\n",
            "distances"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "bb02efa0",
         "metadata": {},
         "outputs": [],
         "source": [
            "fig.savefig(\"word-embeddings-spaces.svg\", bbox_inches=\"tight\")\n",
            "!rsvg-convert -f pdf -o 'word-embeddings-spaces.pdf' 'word-embeddings-spaces.svg'\n",
            "!rm 'word-embeddings-spaces'.svg"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6710e11c",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "# Faiss Search"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7a45bb30",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from rae.openfaiss import FaissIndex\n",
            "\n",
            "faiss_indices = {\n",
            "    enc_name: {model_type: FaissIndex(d=300) for model_type in [\"absolute\", \"relative\"]} for enc_name in ENCODERS\n",
            "}\n",
            "\n",
            "for enc_name, encoder in ENCODERS.items():\n",
            "    latents = encoder.model.get_normed_vectors()\n",
            "    faiss_indices[enc_name][\"absolute\"].add_vectors(\n",
            "        embeddings=list(zip(encoder.model.key_to_index.keys(), latents)), normalize=False\n",
            "    )\n",
            "\n",
            "    rel_attention = RelativeAttention(\n",
            "        n_anchors=NUM_ANCHORS,\n",
            "        n_classes=len(set(TARGETS)),\n",
            "        similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "        values_mode=ValuesMethod.SIMILARITIES,\n",
            "        normalization_mode=NormalizationMode.L2,\n",
            "        #  output_normalization_mode=OutputNormalization.L2,\n",
            "        #  similarities_quantization_mode=quant_mode,\n",
            "        #  similarities_bin_size=bin_size,\n",
            "        #  similarities_num_clusters=bin_size,\n",
            "        absolute_quantization_mode=\"cluster\",\n",
            "        absolute_bin_size=1.5,  # ignored\n",
            "        absolute_num_clusters=1.5,\n",
            "    )\n",
            "    latents = torch.as_tensor(latents, dtype=torch.float32)\n",
            "    relative_representation = rel_attention(x=latents, anchors=anchors_latents[enc_name])\n",
            "    print(relative_representation)\n",
            "\n",
            "    print(enc_name, encoder)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ccb68ff9",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f8794662",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "zip(encoder.model.key_to_index.keys(), encoder.model.get_normed_vectors())"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2132e49f",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "# Optimal transofrm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ca2553b4",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from rae.modules.attention import *\n",
            "\n",
            "ae, _ = parse_checkpoint(\n",
            "    module_class=PL_MODULE,\n",
            "    checkpoint_path=checkpoints[\"mnist\"][\"ae\"][0],\n",
            "    map_location=\"cpu\",\n",
            ")\n",
            "\n",
            "att = RelativeAttention(\n",
            "    n_anchors=anchors_batch.shape,\n",
            "    n_classes=len(set(targets)),\n",
            "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "    values_mode=ValuesMethod.SIMILARITIES,\n",
            "    normalization_mode=NormalizationMode.L2,\n",
            "    output_normalization_mode=OutputNormalization.NONE,\n",
            "    similarities_quantization_mode=None,\n",
            "    similarities_bin_size=None,\n",
            "    # absolute_quantization_mode=AbsoluteQuantizationMode.DIFFERENTIABLE_ROUND,\n",
            "    # absolute_bin_size=bin_size\n",
            ")\n",
            "att_q = RelativeAttention(\n",
            "    n_anchors=anchors_batch.shape,\n",
            "    n_classes=len(set(targets)),\n",
            "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "    values_mode=ValuesMethod.SIMILARITIES,\n",
            "    normalization_mode=NormalizationMode.L2,\n",
            "    output_normalization_mode=OutputNormalization.NONE,\n",
            "    similarities_quantization_mode=SimilaritiesQuantizationMode.CUSTOM_ROUND,\n",
            "    similarities_bin_size=0.1,\n",
            "    # absolute_quantization_mode=AbsoluteQuantizationMode.DIFFERENTIABLE_ROUND,\n",
            "    # absolute_bin_size=0.1\n",
            ")\n",
            "\n",
            "ae.eval()\n",
            "images_z = ae(images_batch)[Output.DEFAULT_LATENT].detach()\n",
            "anchors_z = ae(anchors_batch)[Output.DEFAULT_LATENT].detach()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "684949d6",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "import torch\n",
            "from torch.optim.adam import Adam\n",
            "\n",
            "# Absolut\n",
            "from scipy.stats import ortho_group\n",
            "\n",
            "opt_isometry = torch.tensor(ortho_group.rvs(images_z.shape[-1]), dtype=torch.float, requires_grad=True)\n",
            "opt_shift = torch.zeros(images_z.shape[-1], dtype=torch.float, requires_grad=True)\n",
            "\n",
            "opt = Adam([opt_isometry, opt_shift], lr=1e-4)\n",
            "\n",
            "\n",
            "def transform(x):\n",
            "    return x @ opt_isometry + opt_shift\n",
            "\n",
            "\n",
            "R = 1000\n",
            "Q = 1\n",
            "I = 1000\n",
            "S = 0\n",
            "for i in (bar := tqdm(range(100))):\n",
            "    rel = att(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "    rel_iso = att(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "    rel_dist = F.mse_loss(rel, rel_iso, reduction=\"sum\")\n",
            "    rel_loss = -rel_dist * R\n",
            "\n",
            "    qrel = att_q(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "    qrel_iso = att_q(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "    qrel_dist = F.mse_loss(qrel, rel_iso, reduction=\"sum\")\n",
            "    qrel_loss = qrel_dist * Q\n",
            "\n",
            "    t_temp = opt_isometry @ opt_isometry.T\n",
            "    iso_loss = ((t_temp - t_temp.diag().diag()) ** 2).sum() * I\n",
            "    # iso_loss = (t_temp ** 2 - torch.eye(t_temp.shape[0])).sum() * I\n",
            "    shift_loss = opt_shift.abs().sum() * S\n",
            "    loss = rel_loss + qrel_loss + iso_loss + shift_loss\n",
            "\n",
            "    bar.set_description(f\"Rel: {rel_loss.item():3f} \\t Qua: {qrel_loss.item():3f} \\t  Iso: {iso_loss.item():3f}\")\n",
            "    loss.backward()\n",
            "    opt.step()\n",
            "    opt.zero_grad()\n",
            "\n",
            "rel = att(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "rel_iso = att(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Relative mse:\", F.mse_loss(rel, rel_iso, reduction=\"sum\"))\n",
            "\n",
            "qrel = att_q(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "qrel_iso = att_q(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Quantized mse:\", F.mse_loss(qrel, qrel_iso, reduction=\"sum\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "02bdc2f0",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7ade66dc",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "ae.eval()\n",
            "images_z = ae(images_batch)[Output.DEFAULT_LATENT].detach()\n",
            "anchors_z = ae(anchors_batch)[Output.DEFAULT_LATENT].detach()\n",
            "\n",
            "rel = att(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "rel_iso = att(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Relative mse:\", F.mse_loss(rel, rel_iso, reduction=\"sum\"))\n",
            "\n",
            "qrel = att_q(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "qrel_iso = att_q(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Quantized mse:\", F.mse_loss(qrel, qrel_iso, reduction=\"sum\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f633fbc8",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "qrel"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f74aba10",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "b = torch.as_tensor(0.5)\n",
            "x = torch.linspace(-1, 1, 200)\n",
            "y = x - torch.sin(2 * torch.pi * x) / (2 * torch.pi)\n",
            "\n",
            "a = 1\n",
            "f = 1 / b\n",
            "s = 0\n",
            "y = x - a * torch.cos(2 * torch.pi * f * x + s) / (2 * torch.pi * f)\n",
            "\n",
            "fig, ax = plt.subplots(1, 1, dpi=150)\n",
            "f = ax.plot(\n",
            "    x,\n",
            "    y,\n",
            ")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.13"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
