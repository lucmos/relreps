{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1dee1ca4",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "% load_ext autoreload\n",
            "% autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f3278c18",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "import logging\n",
            "\n",
            "import pandas as pd\n",
            "import torch\n",
            "import torch.nn.functional as F\n",
            "from rae.modules.enumerations import Output\n",
            "from rae.pl_modules.pl_gautoencoder import LightningAutoencoder\n",
            "\n",
            "try:\n",
            "    # be ready for 3.10 when it drops\n",
            "    from enum import StrEnum\n",
            "except ImportError:\n",
            "    from backports.strenum import StrEnum\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "from tueplots import bundles\n",
            "from tueplots import figsizes\n",
            "\n",
            "logging.getLogger().setLevel(logging.ERROR)\n",
            "\n",
            "DEVICE: str = \"cuda\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "99d998ac",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from rae.modules.text.encoder import GensimEncoder\n",
            "\n",
            "ENCODERS = [\n",
            "    GensimEncoder(language=\"en\", lemmatize=False, model_name=model_name)\n",
            "    for model_name in (\n",
            "        \"local_fasttext\",\n",
            "        \"word2vec-google-news-300\",\n",
            "        \"glove-wiki-gigaword-300\",\n",
            "    )\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d13cd066",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "assert len({frozenset(encoder.model.key_to_index.keys()) for encoder in ENCODERS}) == 1"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2e385241",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "import random\n",
            "\n",
            "NUM_ANCHORS = 500\n",
            "NUM_TARGETS = 1000\n",
            "# NUM_WORDS = 20_000\n",
            "WORDS = sorted(ENCODERS[0].model.key_to_index.keys())\n",
            "WORDS = [word for word in WORDS if word.isalpha() and len(word) >= 4]\n",
            "TARGET_WORDS = [\"sea\", \"human\", \"sword\"]  # words to take the neighborhoods from\n",
            "TARGET_WORDS = random.sample(WORDS, 3)\n",
            "print(f\"{TARGET_WORDS=}\")\n",
            "word2index = {word: i for i, word in enumerate(WORDS)}\n",
            "TARGETS = torch.zeros(len(WORDS), device=\"cpu\")\n",
            "target_cluster = [\n",
            "    [word for word, sim in ENCODERS[0].model.most_similar(target_word, topn=NUM_TARGETS)]\n",
            "    for target_word in TARGET_WORDS\n",
            "]\n",
            "\n",
            "valid_words, valid_targets = [], []\n",
            "for i, target_cluster in enumerate(target_cluster):\n",
            "    valid_words.append(TARGET_WORDS[i])\n",
            "    valid_targets.append(i + 1)\n",
            "    for word in target_cluster:\n",
            "        if word in word2index:\n",
            "            valid_words.append(word)\n",
            "            valid_targets.append(i + 1)\n",
            "\n",
            "WORDS = valid_words\n",
            "TARGETS = valid_targets\n",
            "\n",
            "ANCHOR_WORDS = sorted(random.sample(WORDS, NUM_ANCHORS))\n",
            "\n",
            "ANCHOR_WORDS[:10]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "148843f7",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from sklearn.decomposition import PCA\n",
            "\n",
            "\n",
            "def get_latents(words, encoder: GensimEncoder, return_df: bool = True):\n",
            "    latents = torch.tensor([encoder.model.get_vector(word) for word in words], device=DEVICE)\n",
            "\n",
            "    latents2d = latents[:, [0, 1]].cpu()\n",
            "    latents2d = PCA(n_components=2).fit_transform(latents.cpu())\n",
            "    df = None\n",
            "    if return_df:\n",
            "        df = pd.DataFrame(\n",
            "            {\n",
            "                \"x\": latents2d[:, 0].tolist(),\n",
            "                \"y\": latents2d[:, 1].tolist(),\n",
            "                # \"class\": classes,\n",
            "                \"target\": TARGETS,\n",
            "                # \"index\": indexes,\n",
            "            }\n",
            "        )\n",
            "    return df, latents"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ed0388f8",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "# Plot stuff"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "61c6b02f",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "def plot_bg(\n",
            "    ax,\n",
            "    df,\n",
            "    cmap,\n",
            "    norm,\n",
            "    size=0.5,\n",
            "    bg_alpha=0.01,\n",
            "):\n",
            "    \"\"\"Create and return a plot of all our movie embeddings with very low opacity.\n",
            "    (Intended to be used as a basis for further - more prominent - plotting of a\n",
            "    subset of movies. Having the overall shape of the map space in the background is\n",
            "    useful for context.)\n",
            "    \"\"\"\n",
            "    ax.scatter(df.x, df.y, c=cmap(norm(df[\"target\"])), alpha=bg_alpha, s=size)\n",
            "    return ax\n",
            "\n",
            "\n",
            "def hightlight_cluster(\n",
            "    ax,\n",
            "    df,\n",
            "    target,\n",
            "    alpha,\n",
            "    cmap,\n",
            "    norm,\n",
            "    size=0.5,\n",
            "):\n",
            "    cluster_df = df[df[\"target\"] == target]\n",
            "    ax.scatter(cluster_df.x, cluster_df.y, c=cmap(norm(cluster_df[\"target\"])), alpha=alpha, s=size)\n",
            "\n",
            "\n",
            "def plot_latent_space(ax, df, targets, size, cmap, norm, bg_alpha=0.1, alpha=0.5):\n",
            "    ax = plot_bg(ax, df, bg_alpha=bg_alpha, cmap=cmap, norm=norm)\n",
            "    for target in targets:\n",
            "        hightlight_cluster(ax, df, target, alpha=alpha, size=size, cmap=cmap, norm=norm)\n",
            "    return ax\n",
            "\n",
            "\n",
            "LIM = len(ENCODERS)\n",
            "N_ROWS = 1\n",
            "N_COLS = LIM\n",
            "\n",
            "plt.rcParams.update(bundles.icml2022())\n",
            "plt.rcParams.update(figsizes.icml2022_full(ncols=N_COLS, nrows=N_ROWS, height_to_width_ratio=1.0))\n",
            "\n",
            "cmap = plt.cm.get_cmap(\"Set1\", 10)\n",
            "norm = plt.Normalize(min(TARGETS), max(TARGETS))\n",
            "\n",
            "\n",
            "def plot_row(df, title, equal=True, sharey=False, sharex=False, dpi=150):\n",
            "    fig, axes = plt.subplots(dpi=dpi, nrows=N_ROWS, ncols=N_COLS, sharey=sharey, sharex=sharex, squeeze=True)\n",
            "\n",
            "    for j, ax in enumerate(axes):\n",
            "        if j == 0:\n",
            "            ax.set_title(title)\n",
            "        if equal:\n",
            "            ax.set_aspect(\"equal\")\n",
            "        plot_latent_space(ax, df[j], targets=[0, 1], size=0.75, bg_alpha=0.25, alpha=1, cmap=cmap, norm=norm)\n",
            "    return fig"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1d7f4997",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "def latents_distance(latents):\n",
            "    dists = []\n",
            "    for i in range(len(latents)):\n",
            "        for j in range(i + 1, len(latents)):\n",
            "            x = latents[i][1]\n",
            "            y = latents[j][1]\n",
            "            # dist = ((x - y)**2).sum(dim=-1).sqrt().mean()\n",
            "            dist = F.pairwise_distance(x, y, p=torch.inf).mean()\n",
            "            dist = F.mse_loss(x, y, reduction=\"mean\")\n",
            "\n",
            "            # dist = ((x - y) ** 2).mean(dim=-1).mean()\n",
            "            dists.append(f\"{i}-{j}: {dist}\")\n",
            "    return \" \".join(dists)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "13da9076",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "## AE"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c47ff8cd",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "ae_latents = []\n",
            "anchors_latents = []\n",
            "for encoder in ENCODERS:\n",
            "    df, latents = get_latents(words=WORDS, encoder=encoder, return_df=True)\n",
            "    _, a_latents = get_latents(words=ANCHOR_WORDS, encoder=encoder, return_df=False)\n",
            "    ae_latents.append((df, latents))\n",
            "    anchors_latents.append(a_latents)\n",
            "\n",
            "import copy\n",
            "\n",
            "original_ae_latents = copy.deepcopy(ae_latents)\n",
            "original_anchor_latents = copy.deepcopy(anchors_latents)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a75aa6db",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "f = plot_row([df for df, _ in original_ae_latents[:LIM]], \"AE\", True, True, True)\n",
            "latents_distance(ae_latents[:LIM])"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e466ea65",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "## Rel Attention Quantized"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fdf7c14e",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from sklearn.decomposition import PCA\n",
            "\n",
            "f = plot_row([df for df, _ in ae_latents[:LIM]], f\"AE: {latents_distance(ae_latents[:LIM])}\", True, True, True)\n",
            "\n",
            "# Relative\n",
            "from rae.modules.attention import *\n",
            "\n",
            "# Quantized\n",
            "for quant_mode, bin_size in (\n",
            "    (None, None),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.0001),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.05),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.1),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.3),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.4),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.5),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.6),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.7),\n",
            "    (SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND, 0.8),\n",
            "):\n",
            "    rel_latents = []\n",
            "    rel_attention = RelativeAttention(\n",
            "        n_anchors=NUM_ANCHORS,\n",
            "        n_classes=len(set(TARGETS)),\n",
            "        similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "        values_mode=ValuesMethod.SIMILARITIES,\n",
            "        normalization_mode=NormalizationMode.L2,\n",
            "        # output_normalization_mode=OutputNormalization.NONE,\n",
            "        similarities_quantization_mode=quant_mode,\n",
            "        similarities_bin_size=bin_size,\n",
            "        # absolute_quantization_mode=quant_mode,\n",
            "        # absolute_bin_size=bin_size\n",
            "        hidden_features=None,\n",
            "        transform_elements=None,\n",
            "        in_features=None,\n",
            "        values_self_attention_nhead=None,\n",
            "        similarities_aggregation_mode=None,\n",
            "        similarities_aggregation_n_groups=None,\n",
            "        anchors_sampling_mode=None,\n",
            "        n_anchors_sampling_per_class=None,\n",
            "    )\n",
            "    assert sum(x.numel() for x in rel_attention.parameters()) == 0\n",
            "    for (_, latents), a_latents in zip(ae_latents, anchors_latents):\n",
            "        rel = rel_attention(x=latents, anchors=a_latents)[AttentionOutput.SIMILARITIES]\n",
            "        rellatents2d = rel[:, [0, 1]]\n",
            "        # rellatents2d = PCA(n_components=2).fit_transform(rel.cpu().detach())\n",
            "        df = pd.DataFrame(\n",
            "            {\n",
            "                \"x\": rellatents2d[:, 0].tolist(),\n",
            "                \"y\": rellatents2d[:, 1].tolist(),\n",
            "                # \"class\": classes,\n",
            "                \"target\": TARGETS,\n",
            "                # \"index\": indexes,\n",
            "            }\n",
            "        )\n",
            "        rel_latents.append((df, rel))\n",
            "    f = plot_row(\n",
            "        [df for df, _ in rel_latents[:LIM]],\n",
            "        f\"QAtt, bin size: {bin_size}: {latents_distance(rel_latents[:LIM])}\",\n",
            "        True,\n",
            "        True,\n",
            "        True,\n",
            "    )"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2132e49f",
         "metadata": {
            "pycharm": {
               "name": "#%% md\n"
            }
         },
         "source": [
            "# Optimal transofrm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ca2553b4",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from rae.modules.attention import *\n",
            "\n",
            "ae, _ = parse_checkpoint(\n",
            "    module_class=PL_MODULE,\n",
            "    checkpoint_path=checkpoints[\"mnist\"][\"ae\"][0],\n",
            "    map_location=\"cpu\",\n",
            ")\n",
            "\n",
            "att = RelativeAttention(\n",
            "    n_anchors=anchors_batch.shape,\n",
            "    n_classes=len(set(targets)),\n",
            "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "    values_mode=ValuesMethod.SIMILARITIES,\n",
            "    normalization_mode=NormalizationMode.L2,\n",
            "    output_normalization_mode=OutputNormalization.NONE,\n",
            "    similarities_quantization_mode=None,\n",
            "    similarities_bin_size=None,\n",
            "    # absolute_quantization_mode=AbsoluteQuantizationMode.DIFFERENTIABLE_ROUND,\n",
            "    # absolute_bin_size=bin_size\n",
            ")\n",
            "att_q = RelativeAttention(\n",
            "    n_anchors=anchors_batch.shape,\n",
            "    n_classes=len(set(targets)),\n",
            "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
            "    values_mode=ValuesMethod.SIMILARITIES,\n",
            "    normalization_mode=NormalizationMode.L2,\n",
            "    output_normalization_mode=OutputNormalization.NONE,\n",
            "    similarities_quantization_mode=SimilaritiesQuantizationMode.CUSTOM_ROUND,\n",
            "    similarities_bin_size=0.1,\n",
            "    # absolute_quantization_mode=AbsoluteQuantizationMode.DIFFERENTIABLE_ROUND,\n",
            "    # absolute_bin_size=0.1\n",
            ")\n",
            "\n",
            "ae.eval()\n",
            "images_z = ae(images_batch)[Output.DEFAULT_LATENT].detach()\n",
            "anchors_z = ae(anchors_batch)[Output.DEFAULT_LATENT].detach()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "684949d6",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "import torch\n",
            "from torch.optim.adam import Adam\n",
            "\n",
            "# Absolut\n",
            "from scipy.stats import ortho_group\n",
            "\n",
            "opt_isometry = torch.tensor(ortho_group.rvs(images_z.shape[-1]), dtype=torch.float, requires_grad=True)\n",
            "opt_shift = torch.zeros(images_z.shape[-1], dtype=torch.float, requires_grad=True)\n",
            "\n",
            "opt = Adam([opt_isometry, opt_shift], lr=1e-4)\n",
            "\n",
            "\n",
            "def transform(x):\n",
            "    return x @ opt_isometry + opt_shift\n",
            "\n",
            "\n",
            "R = 1000\n",
            "Q = 1\n",
            "I = 1000\n",
            "S = 0\n",
            "for i in (bar := tqdm(range(100))):\n",
            "    rel = att(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "    rel_iso = att(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "    rel_dist = F.mse_loss(rel, rel_iso, reduction=\"sum\")\n",
            "    rel_loss = -rel_dist * R\n",
            "\n",
            "    qrel = att_q(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "    qrel_iso = att_q(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "    qrel_dist = F.mse_loss(qrel, rel_iso, reduction=\"sum\")\n",
            "    qrel_loss = qrel_dist * Q\n",
            "\n",
            "    t_temp = opt_isometry @ opt_isometry.T\n",
            "    iso_loss = ((t_temp - t_temp.diag().diag()) ** 2).sum() * I\n",
            "    # iso_loss = (t_temp ** 2 - torch.eye(t_temp.shape[0])).sum() * I\n",
            "    shift_loss = opt_shift.abs().sum() * S\n",
            "    loss = rel_loss + qrel_loss + iso_loss + shift_loss\n",
            "\n",
            "    bar.set_description(f\"Rel: {rel_loss.item():3f} \\t Qua: {qrel_loss.item():3f} \\t  Iso: {iso_loss.item():3f}\")\n",
            "    loss.backward()\n",
            "    opt.step()\n",
            "    opt.zero_grad()\n",
            "\n",
            "rel = att(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "rel_iso = att(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Relative mse:\", F.mse_loss(rel, rel_iso, reduction=\"sum\"))\n",
            "\n",
            "qrel = att_q(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "qrel_iso = att_q(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Quantized mse:\", F.mse_loss(qrel, qrel_iso, reduction=\"sum\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "02bdc2f0",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7ade66dc",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "ae.eval()\n",
            "images_z = ae(images_batch)[Output.DEFAULT_LATENT].detach()\n",
            "anchors_z = ae(anchors_batch)[Output.DEFAULT_LATENT].detach()\n",
            "\n",
            "rel = att(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "rel_iso = att(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Relative mse:\", F.mse_loss(rel, rel_iso, reduction=\"sum\"))\n",
            "\n",
            "qrel = att_q(x=images_z, anchors=anchors_z)[AttentionOutput.SIMILARITIES]\n",
            "qrel_iso = att_q(x=transform(images_z), anchors=transform(anchors_z))[AttentionOutput.SIMILARITIES]\n",
            "print(\"Quantized mse:\", F.mse_loss(qrel, qrel_iso, reduction=\"sum\"))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f633fbc8",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "qrel"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f74aba10",
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "b = torch.as_tensor(0.5)\n",
            "x = torch.linspace(-1, 1, 200)\n",
            "y = x - torch.sin(2 * torch.pi * x) / (2 * torch.pi)\n",
            "\n",
            "a = 1\n",
            "f = 1 / b\n",
            "s = 0\n",
            "y = x - a * torch.cos(2 * torch.pi * f * x + s) / (2 * torch.pi * f)\n",
            "\n",
            "fig, ax = plt.subplots(1, 1, dpi=150)\n",
            "f = ax.plot(\n",
            "    x,\n",
            "    y,\n",
            ")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.9.2 64-bit",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.2"
      },
      "vscode": {
         "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
