{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Mapping, Tuple\n",
    "\n",
    "from rae.openfaiss import FaissIndex\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from pytorch_lightning import seed_everything\n",
    "import numpy as np\n",
    "from rae.modules.attention import *\n",
    "from rae.utils.utils import StrEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rae.modules.text.encoder import GensimEncoder\n",
    "\n",
    "ENCODERS = {\n",
    "    model_name: GensimEncoder(language=\"en\", lemmatize=False, model_name=model_name)\n",
    "    for model_name in (\n",
    "        \"local_fasttext\",\n",
    "        \"word2vec-google-news-300\",\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(list(ENCODERS.values())[0].model.key_to_index.keys())\n",
    "all_words = [\n",
    "    word for word in all_words[400:] if word.isalpha() and len(word) >= 4\n",
    "]  # skip stopwords (first ~400 words) and filter out non-alpha and short words\n",
    "# random.shuffle(random_words)\n",
    "SEARCH_WORDS = all_words[:20_000]\n",
    "SEARCH_WORDS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_ANCHORS_NUM = 300\n",
    "NUM_SEEDS: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(words, encoder: GensimEncoder):\n",
    "    latents = torch.tensor([encoder.model.get_vector(word) for word in words], device=DEVICE)\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "\n",
    "def build_neighborhood(\n",
    "    enc_type2enc_name2faiss_index: Mapping[str, Mapping[str, FaissIndex]]\n",
    ") -> Mapping[str, Mapping[Tuple[str, str], Mapping[str, Sequence[str]]]]:\n",
    "    enc_type2enc_names2word2topk = {enc_type: {} for enc_type in (\"absolute\", \"relative\")}\n",
    "\n",
    "    for enc_type, enc_name2faiss_index in enc_type2enc_name2faiss_index.items():\n",
    "        for enc_name1, enc_name2 in itertools.product(enc_name2faiss_index.keys(), repeat=2):\n",
    "            faiss_index1 = enc_name2faiss_index[enc_name1]\n",
    "            faiss_index2 = enc_name2faiss_index[enc_name2]\n",
    "\n",
    "            enc1_vectors = np.asarray(faiss_index1.reconstruct_n(SEARCH_WORDS), dtype=\"float32\")\n",
    "            enc2_neighbors = faiss_index2.search_by_vectors(\n",
    "                query_vectors=enc1_vectors, k_most_similar=K, normalize=False\n",
    "            )\n",
    "\n",
    "            enc_type2enc_names2word2topk[enc_type][(enc_name1, enc_name2)] = {\n",
    "                word: topk for word, topk in zip(SEARCH_WORDS, enc2_neighbors)\n",
    "            }\n",
    "\n",
    "    return enc_type2enc_names2word2topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(enc_type2enc_names2word2topk: Mapping[str, Mapping[Tuple[str, str], Mapping[str, Sequence[str]]]]):\n",
    "    performance = {key: [] for key in (\"src_enc\", \"tgt_enc\", \"enc_type\", \"topk_jaccard\", \"mrr\")}\n",
    "\n",
    "    for enc_type, enc_names2word2topk in enc_type2enc_names2word2topk.items():\n",
    "        for (enc_name1, enc_name2), word2topk in enc_names2word2topk.items():\n",
    "            target_words = {\n",
    "                search_word: set(enc_names2word2topk[(enc_name2, enc_name2)][search_word].keys())\n",
    "                for search_word in SEARCH_WORDS\n",
    "            }\n",
    "            actual_words = {\n",
    "                search_word: set(enc_names2word2topk[(enc_name1, enc_name2)][search_word].keys())\n",
    "                for search_word in SEARCH_WORDS\n",
    "            }\n",
    "\n",
    "            topk_jaccard = {\n",
    "                search_word: len(set.intersection(target_words[search_word], actual_words[search_word]))\n",
    "                / len(set.union(target_words[search_word], actual_words[search_word]))\n",
    "                for search_word in SEARCH_WORDS\n",
    "            }\n",
    "            topk_jaccard = np.mean(list(topk_jaccard.values()))\n",
    "\n",
    "            search_word2word2rank = {\n",
    "                search_word: {key: index for index, key in enumerate(word2sim.keys(), start=1)}\n",
    "                for search_word, word2sim in enc_names2word2topk[(enc_name1, enc_name2)].items()\n",
    "            }\n",
    "            mrr = {\n",
    "                search_word: (\n",
    "                    #                 word2rank.get(search_word, K)\n",
    "                    0\n",
    "                    if search_word not in word2rank\n",
    "                    else 1 / word2rank[search_word]\n",
    "                )\n",
    "                for search_word, word2rank in search_word2word2rank.items()\n",
    "            }\n",
    "            mrr = np.mean(list(mrr.values()))\n",
    "\n",
    "            # semantic_horizon = []\n",
    "            # for search_word, neighbors in actual_words.items():\n",
    "            #     neighbor2ranking = {\n",
    "            #         neighbor: {\n",
    "            #             key: index\n",
    "            #             for index, key in enumerate(\n",
    "            #                 enc_type2enc_names2word2topk[\"absolute\"][(enc_name2, enc_name2)][neighbor].keys(), start=1\n",
    "            #             )\n",
    "            #         }\n",
    "            #         for neighbor in neighbors\n",
    "            #     }\n",
    "            #     neighbor2mrr = {\n",
    "            #         neighbor: (\n",
    "            #             #                 topk.get(search_word, K)\n",
    "            #             0\n",
    "            #             if search_word not in ranking\n",
    "            #             else 1 / ranking[search_word]\n",
    "            #         )\n",
    "            #         for neighbor, ranking in neighbor2ranking.items()\n",
    "            #     }\n",
    "            #     semantic_horizon.append(np.mean(list(neighbor2mrr.values())))\n",
    "            #\n",
    "            # semantic_horizon = np.mean(semantic_horizon)\n",
    "\n",
    "            performance[\"src_enc\"].append(enc_name1)\n",
    "            performance[\"tgt_enc\"].append(enc_name2)\n",
    "            performance[\"enc_type\"].append(enc_type)\n",
    "            performance[\"topk_jaccard\"].append(topk_jaccard)\n",
    "            performance[\"mrr\"].append(mrr)\n",
    "\n",
    "    return pd.DataFrame(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "rel_attention = RelativeAttention(\n",
    "    n_anchors=RETRIEVAL_ANCHORS_NUM,\n",
    "    n_classes=None,\n",
    "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
    "    values_mode=ValuesMethod.SIMILARITIES,\n",
    "    normalization_mode=NormalizationMode.L2,\n",
    "    #  output_normalization_mode=OutputNormalization.L2,\n",
    "    #          similarities_quantization_mode='differentiable_round',\n",
    "    #          similarities_bin_size=0.01,\n",
    "    #          similarities_num_clusters=,\n",
    "    #         absolute_quantization_mode=\"cluster\",\n",
    "    #         absolute_bin_size=2,  # ignored\n",
    "    #         absolute_num_clusters=2,\n",
    ")\n",
    "\n",
    "performance = []\n",
    "for seed in (pbar := tqdm(range(NUM_SEEDS))):\n",
    "    enc_type2enc_name2faiss_index = {\n",
    "        enc_type: {\n",
    "            enc_name: FaissIndex(d=300 if enc_type == \"absolute\" else RETRIEVAL_ANCHORS_NUM)\n",
    "            for enc_name in ENCODERS.keys()\n",
    "        }\n",
    "        for enc_type in (\"absolute\", \"relative\")\n",
    "    }\n",
    "\n",
    "    for enc_name, encoder in ENCODERS.items():\n",
    "        pbar.set_description(f\"seed={seed}/{NUM_SEEDS} encoder={enc_name}\")\n",
    "\n",
    "        seed_everything(seed)\n",
    "        seed_anchors: Sequence[str] = sorted(random.sample(SEARCH_WORDS, RETRIEVAL_ANCHORS_NUM))\n",
    "        latents = encoder.model.vectors_for_all(keys=SEARCH_WORDS).vectors\n",
    "        enc_type2enc_name2faiss_index[\"absolute\"][enc_name].add_vectors(\n",
    "            embeddings=list(zip(SEARCH_WORDS, latents)), normalize=True\n",
    "        )\n",
    "\n",
    "        anchors: torch.Tensor = get_latents(words=seed_anchors, encoder=encoder)\n",
    "        latents = torch.as_tensor(latents, dtype=torch.float32)\n",
    "        relative_representation = rel_attention(x=latents, anchors=anchors.cpu())\n",
    "\n",
    "        enc_type2enc_name2faiss_index[\"relative\"][enc_name].add_vectors(\n",
    "            embeddings=list(zip(SEARCH_WORDS, relative_representation[AttentionOutput.SIMILARITIES].cpu().numpy())),\n",
    "            normalize=True,\n",
    "        )\n",
    "\n",
    "    seed_neighborhoods = build_neighborhood(enc_type2enc_name2faiss_index=enc_type2enc_name2faiss_index)\n",
    "    seed_performance = evaluate(enc_type2enc_names2word2topk=seed_neighborhoods)\n",
    "    seed_performance[\"seed\"] = seed\n",
    "    performance.append(seed_performance)\n",
    "\n",
    "performance = pd.concat(performance)\n",
    "performance.to_csv(\n",
    "    PROJECT_ROOT / \"experiments\" / \"fig:latent-rotation-comparison\" / \"retrieval_performance.tsv\", sep=\"\\t\"\n",
    ")\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.read_csv(\n",
    "    PROJECT_ROOT / \"experiments\" / \"fig:latent-rotation-comparison\" / \"retrieval_performance.tsv\", sep=\"\\t\"\n",
    ")\n",
    "performance_df\n",
    "performance_df.groupby([\"src_enc\", \"tgt_enc\", \"enc_type\"]).aggregate([np.mean, np.std, \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
