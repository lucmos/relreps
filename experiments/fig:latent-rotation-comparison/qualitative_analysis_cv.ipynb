{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rae import PROJECT_ROOT\n",
    "import random\n",
    "\n",
    "try:\n",
    "    # be ready for 3.10 when it drops\n",
    "    from enum import StrEnum\n",
    "except ImportError:\n",
    "    from backports.strenum import StrEnum\n",
    "from pytorch_lightning import seed_everything\n",
    "import matplotlib.pyplot as plt\n",
    "from tueplots import bundles\n",
    "from tueplots import figsizes\n",
    "import random\n",
    "\n",
    "DEVICE: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "EXPERIMENT_DIR: Path = PROJECT_ROOT / \"experiments\" / \"fig:latent-rotation-comparison\"\n",
    "VISION_DATASET_DIR: Path = EXPERIMENT_DIR / \"encoded_data\" / \"cv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "ENCODERS = (\n",
    "    \"vit_base_patch16_224\",\n",
    "    \"rexnet_100\",\n",
    "    \"vit_base_resnet50_384\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CACHE_DIR = os.getenv(\"HF_DATASETS_CACHE\")\n",
    "CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, List\n",
    "from PIL.Image import Image\n",
    "\n",
    "\n",
    "def encode_field(batch, src_field: str, tgt_field: str, transformation):\n",
    "    src_data = batch[src_field]\n",
    "    transformed = transformation(src_data)\n",
    "\n",
    "    return {tgt_field: transformed}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def image_encode(images: Sequence[Image], transform, encoder):\n",
    "    images: List[torch.Tensor] = [transform(image.convert(\"RGB\")) for image in images]\n",
    "    images: torch.Tensor = torch.stack(images, dim=0).to(DEVICE)\n",
    "    encoding = encoder(images)\n",
    "\n",
    "    return list(encoding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import functools\n",
    "from timm.data import resolve_data_config\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "\n",
    "from timm.data import create_transform\n",
    "\n",
    "USE_CACHED: bool = True\n",
    "if not VISION_DATASET_DIR.exists() or not USE_CACHED:\n",
    "\n",
    "    def get_dataset(split: str, perc: float):\n",
    "        assert 0 < perc <= 1\n",
    "        dataset = load_dataset(\n",
    "            \"imagenet-1k\",\n",
    "            split=split,\n",
    "            data_dir=PROJECT_ROOT / \"data\" / \"imagenet1k\",\n",
    "            cache_dir=CACHE_DIR,\n",
    "            use_auth_token=True,\n",
    "        )\n",
    "        seed_everything(42)\n",
    "\n",
    "        # Select a random subset\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        assert indices[:10] == [3813, 99183, 80737, 65674, 53670, 97104, 38743, 36888, 75154, 1326], indices[:10]\n",
    "        indices = indices[: int(len(indices) * perc)]\n",
    "        dataset = dataset.select(indices)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    data: Dataset = get_dataset(split=\"test\", perc=0.2)\n",
    "else:\n",
    "    data: Dataset = load_from_disk(dataset_path=str(VISION_DATASET_DIR))\n",
    "\n",
    "print(data)\n",
    "\n",
    "FORCE_RECOMPUTE: bool = False\n",
    "missing_encoders = [encoder for encoder in ENCODERS if FORCE_RECOMPUTE or encoder not in data.column_names]\n",
    "for encoder_name in tqdm(missing_encoders):\n",
    "    tgt_field: str = encoder_name\n",
    "    encoder = timm.create_model(encoder_name, pretrained=True, num_classes=0).requires_grad_(False).eval().to(DEVICE)\n",
    "    config = resolve_data_config({}, model=encoder)\n",
    "    transform = create_transform(**config)\n",
    "\n",
    "    data = data.map(\n",
    "        functools.partial(\n",
    "            encode_field,\n",
    "            src_field=\"image\",\n",
    "            tgt_field=tgt_field,\n",
    "            transformation=functools.partial(\n",
    "                image_encode,\n",
    "                transform=transform,\n",
    "                encoder=encoder,\n",
    "            ),\n",
    "        ),\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        desc=f\"{encoder_name}\",\n",
    "    )\n",
    "    encoder = encoder.cpu()\n",
    "\n",
    "if len(missing_encoders) > 0:\n",
    "    data.save_to_disk(str(VISION_DATASET_DIR))\n",
    "\n",
    "\n",
    "def data_transform(x):\n",
    "    result = {encoder: torch.as_tensor(x[encoder]) for encoder in ENCODERS}\n",
    "    result[\"image\"] = x[\"image\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "data.set_format(columns=ENCODERS, output_all_columns=True, type=\"torch\")\n",
    "# data.add_faiss_index(column=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHORS_NUM: int = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(words, encoder):\n",
    "    return data.select([int(x) for x in words])[encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from rae.openfaiss import FaissIndex\n",
    "from rae.modules.attention import *\n",
    "from torch_cluster import fps\n",
    "\n",
    "rel_proj = RelativeAttention(\n",
    "    n_anchors=ANCHORS_NUM,\n",
    "    n_classes=None,\n",
    "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
    "    values_mode=ValuesMethod.SIMILARITIES,\n",
    "    normalization_mode=NormalizationMode.L2,\n",
    "    #  output_normalization_mode=OutputNormalization.L2,\n",
    "    #          similarities_quantization_mode='differentiable_round',\n",
    "    #          similarities_bin_size=0.01,\n",
    "    #          similarities_num_clusters=,\n",
    "    #         absolute_quantization_mode=\"cluster\",\n",
    "    #         absolute_bin_size=2,  # ignored\n",
    "    #         absolute_num_clusters=2,\n",
    ")\n",
    "\n",
    "\n",
    "class LatentSpace:\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding_type: str,\n",
    "        encoder: str,\n",
    "        vectors: torch.Tensor,\n",
    "        ids: Sequence[int],\n",
    "    ):\n",
    "        self.encoding_type: str = encoding_type\n",
    "        self.vectors: torch.Tensor = vectors\n",
    "        self.ids: Sequence[int] = ids\n",
    "        self.encoder: str = encoder\n",
    "\n",
    "        self._cached_anchors = {}\n",
    "        self._cache_index = None\n",
    "\n",
    "    def to_faiss(self) -> FaissIndex:\n",
    "        if self._cache_index is not None:\n",
    "            return self._cache_index\n",
    "        index: FaissIndex = FaissIndex(d=self.vectors.size(1))\n",
    "\n",
    "        index.add_vectors(\n",
    "            embeddings=list(zip([str(sample_id) for sample_id in self.ids], self.vectors.cpu().numpy())),\n",
    "            normalize=True,\n",
    "        )\n",
    "\n",
    "        self._cache_index = index\n",
    "        return index\n",
    "\n",
    "    def to_relative(\n",
    "        self, anchor_choice: str = None, seed: int = None, anchors: Optional[Sequence[str]] = None\n",
    "    ) -> \"RelativeSpace\":\n",
    "        assert self.encoding_type != \"relative\"  # TODO: for now\n",
    "        anchors = self.get_anchors(anchor_choice=anchor_choice, seed=seed) if anchors is None else anchors\n",
    "\n",
    "        anchor_latents: torch.Tensor = get_latents(words=anchors, encoder=self.encoder)\n",
    "\n",
    "        relative_vectors = rel_proj(x=self.vectors, anchors=anchor_latents.cpu())[AttentionOutput.SIMILARITIES].cpu()\n",
    "        return RelativeSpace(vectors=relative_vectors, encoder=self.encoder, anchors=anchors, ids=self.ids)\n",
    "\n",
    "    def get_anchors(self, anchor_choice: str, seed: int) -> Sequence[str]:\n",
    "        key = (seed, anchor_choice)\n",
    "        if key in self._cached_anchors:\n",
    "            # print(f\"Cache match: {key} in {self._cached_anchors.keys()}\")\n",
    "            return self._cached_anchors[key]\n",
    "        else:\n",
    "            # print(f\"Cache miss: {key} not in {self._cached_anchors.keys()}\")\n",
    "            pass\n",
    "        # Select anchors\n",
    "        seed_everything(seed)\n",
    "        if anchor_choice == \"uniform\" or anchor_choice.startswith(\"top_\"):\n",
    "            limit: int = len(self.ids) if anchor_choice == \"uniform\" else int(anchor_choice[4:])\n",
    "            anchor_set: Sequence[int] = random.sample(self.ids[:limit], ANCHORS_NUM)\n",
    "        elif anchor_choice == \"fps\":\n",
    "            anchor_fps = get_latents(words=self.ids, encoder=self.encoder)\n",
    "            anchor_fps = F.normalize(anchor_fps, p=2, dim=-1)\n",
    "            anchor_fps = fps(anchor_fps, random_start=True, ratio=ANCHORS_NUM / len(self.ids))\n",
    "            anchor_set: Sequence[int] = [self.ids[word_index] for word_index in anchor_fps.cpu().tolist()]\n",
    "        elif anchor_choice == \"kmeans\":\n",
    "            vectors = F.normalize(get_latents(words=self.ids, encoder=self.encoder), p=2)\n",
    "            clustered = KMeans(n_clusters=ANCHORS_NUM).fit_predict(vectors.cpu().numpy())\n",
    "\n",
    "            all_targets = sorted(set(clustered))\n",
    "            cluster2embeddings = {target: vectors[clustered == target] for target in all_targets}\n",
    "            cluster2centroid = {\n",
    "                cluster: centroid.mean(dim=0).cpu().numpy() for cluster, centroid in cluster2embeddings.items()\n",
    "            }\n",
    "            centroids = np.array(list(cluster2centroid.values()), dtype=\"float32\")\n",
    "\n",
    "            index: FaissIndex = FaissIndex(d=vectors.shape[1])\n",
    "            index.add_vectors(\n",
    "                list(zip([str(sample_id) for sample_id in self.ids], vectors.cpu().numpy())), normalize=False\n",
    "            )\n",
    "            centroids = index.search_by_vectors(query_vectors=centroids, k_most_similar=1, normalize=True)\n",
    "\n",
    "            anchor_set = [list(word2score.keys())[0] for word2score in centroids]\n",
    "        else:\n",
    "            assert NotImplementedError\n",
    "\n",
    "        result = sorted(anchor_set)\n",
    "        self._cached_anchors[key] = result\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class RelativeSpace(LatentSpace):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vectors: torch.Tensor,\n",
    "        ids: Sequence[int],\n",
    "        anchors: Sequence[str],\n",
    "        encoder: str = None,\n",
    "    ):\n",
    "        super().__init__(encoding_type=\"relative\", vectors=vectors, encoder=encoder, ids=ids)\n",
    "        self.anchors: Sequence[str] = anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_ENCODERS = (\n",
    "    \"vit_base_patch16_224\",\n",
    "    \"vit_base_resnet50_384\",\n",
    "    \"rexnet_100\",\n",
    ")\n",
    "assert all(x in data.column_names for x in BENCHMARK_ENCODERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED: int = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "NUM_ANCHORS = 768\n",
    "NUM_TARGETS = 200\n",
    "NUM_CLUSTERS = 4\n",
    "ALL_IDS = list(range(len(data)))\n",
    "TARGET_IDS = random.sample(ALL_IDS, NUM_CLUSTERS)  # words to take the neighborhoods from\n",
    "print(f\"{TARGET_IDS=}\")\n",
    "\n",
    "ids = list(range(len(data)))\n",
    "pivot_space = LatentSpace(\n",
    "    encoding_type=\"absolute\", encoder=BENCHMARK_ENCODERS[0], vectors=data.select(ids)[BENCHMARK_ENCODERS[0]], ids=ids\n",
    ")\n",
    "faiss_index = pivot_space.to_faiss()\n",
    "\n",
    "\n",
    "target_cluster = [\n",
    "    [int(x) for x in neighbor2score.keys()]\n",
    "    for word, neighbor2score in faiss_index.search_by_keys(query=TARGET_IDS, k_most_similar=NUM_TARGETS * 2).items()\n",
    "]\n",
    "\n",
    "valid_words, valid_targets = [], []\n",
    "id2target = {}\n",
    "for i, target_cluster in enumerate(target_cluster):\n",
    "    id2target[TARGET_IDS[i]] = i\n",
    "    cluster_ids = [cluster_id for cluster_id in target_cluster if cluster_id not in id2target]\n",
    "    for word_id in cluster_ids[:NUM_TARGETS]:\n",
    "        id2target[word_id] = i\n",
    "\n",
    "BENCHMARK_IDS = list(id2target.keys())\n",
    "print(len(BENCHMARK_IDS))\n",
    "TARGETS = list(id2target.values())\n",
    "# TARGETS = valid_targets\n",
    "\n",
    "ANCHOR_IDS = pivot_space.get_anchors(anchor_choice=\"uniform\", seed=SEED)  # TODO: stratified\n",
    "\n",
    "ANCHOR_IDS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from enum import auto\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def latents_distance(latents):\n",
    "    assert len(latents) == 2\n",
    "    # for x in latents:\n",
    "    #     assert x.shape[1] == 300\n",
    "\n",
    "    dist = F.pairwise_distance(latents[0], latents[1], p=2).mean()\n",
    "    return f\"{dist:.2f}\"\n",
    "\n",
    "\n",
    "class Reduction(StrEnum):\n",
    "    PCA = auto()\n",
    "    TSNE = auto()\n",
    "    UMAP = auto()\n",
    "    FIRST_DIMS = auto()\n",
    "\n",
    "\n",
    "def to_df(latents, mode: Reduction = \"pca\"):\n",
    "    latents2d: np.ndarray\n",
    "    if mode == Reduction.PCA:\n",
    "        latents2d = PCA(n_components=2).fit_transform(latents.cpu())\n",
    "    elif mode == Reduction.FIRST_DIMS:\n",
    "        latents2d = latents[:, [0, 1]].cpu().numpy()\n",
    "    elif mode == Reduction.TSNE:\n",
    "        latents2d = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", random_state=42).fit_transform(latents.cpu())\n",
    "    # elif mode == Reduction.UMAP:\n",
    "    #     latents2d = UMAP(n_components=2).fit_transform(latents.cpu())\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    min_latent: torch.Tensor = latents2d.min(axis=0)\n",
    "    max_latent: torch.Tensor = latents2d.max(axis=0)\n",
    "    latents2d = (latents2d - min_latent) / (max_latent - min_latent)\n",
    "\n",
    "    # F.normalize(latents2d, p=2, dim=-1)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": latents2d[:, 0].tolist(),\n",
    "            \"y\": latents2d[:, 1].tolist(),\n",
    "            \"target\": TARGETS,\n",
    "        }\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bg(\n",
    "    ax,\n",
    "    df,\n",
    "    cmap,\n",
    "    norm,\n",
    "    size,\n",
    "    bg_alpha,\n",
    "):\n",
    "    \"\"\"Create and return a plot of all our movie embeddings with very low opacity.\n",
    "    (Intended to be used as a basis for further - more prominent - plotting of a\n",
    "    subset of movies. Having the overall shape of the map space in the background is\n",
    "    useful for context.)\n",
    "    \"\"\"\n",
    "    ax.scatter(df.x, df.y, c=cmap(norm(df[\"target\"])), alpha=bg_alpha, s=size)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def hightlight_cluster(\n",
    "    ax,\n",
    "    df,\n",
    "    target,\n",
    "    alpha,\n",
    "    cmap,\n",
    "    norm,\n",
    "    size=0.5,\n",
    "):\n",
    "    cluster_df = df[df[\"target\"] == target]\n",
    "    ax.scatter(cluster_df.x, cluster_df.y, c=cmap(norm(cluster_df[\"target\"])), alpha=alpha, s=size)\n",
    "\n",
    "\n",
    "def plot_latent_space(ax, df, targets, size, cmap, norm, bg_alpha, alpha):\n",
    "    ax = plot_bg(ax, df, bg_alpha=bg_alpha, cmap=cmap, norm=norm, size=size)\n",
    "    for target in targets:\n",
    "        hightlight_cluster(ax, df, target, alpha=alpha, size=size, cmap=cmap, norm=norm)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_latents = {}\n",
    "anchors_latents = {}\n",
    "for encoder in BENCHMARK_ENCODERS:\n",
    "    print(\"encoder\", encoder)\n",
    "    ae_latents[encoder] = get_latents(words=BENCHMARK_IDS, encoder=encoder)\n",
    "    anchors_latents[encoder] = get_latents(words=ANCHOR_IDS, encoder=encoder)\n",
    "\n",
    "import copy\n",
    "\n",
    "original_ae_latents = copy.deepcopy(ae_latents)\n",
    "original_anchor_latents = copy.deepcopy(anchors_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCTION = Reduction.FIRST_DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from rae.modules.attention import *\n",
    "\n",
    "\n",
    "col_config = ((None, None),)\n",
    "N_ROWS = len(BENCHMARK_ENCODERS)\n",
    "N_COLS = len(col_config) + 1\n",
    "print(\n",
    "    N_ROWS,\n",
    "    N_COLS,\n",
    ")\n",
    "plt.rcParams.update(bundles.icml2022())\n",
    "plt.rcParams.update(figsizes.icml2022_full(ncols=N_COLS, nrows=N_ROWS, height_to_width_ratio=1.0))\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "num_colors = len(TARGET_IDS)\n",
    "cmap = mpl.colors.ListedColormap(plt.cm.get_cmap(\"Set1\", 10).colors[:num_colors], name=\"rgb\", N=num_colors)\n",
    "norm = plt.Normalize(min(TARGETS), max(TARGETS))\n",
    "\n",
    "fig, axes = plt.subplots(dpi=300, nrows=N_ROWS, ncols=N_COLS, sharey=True, sharex=True, squeeze=True)\n",
    "\n",
    "S = 7\n",
    "BG_ALPHA = 0.35\n",
    "ALPHA = 0.5\n",
    "\n",
    "TARGET_HIGHLIGHT = [1]\n",
    "for ax_encoders, (_, latents) in zip(axes[0], ae_latents.items()):\n",
    "    plot_latent_space(\n",
    "        ax_encoders,\n",
    "        to_df(latents, mode=REDUCTION),\n",
    "        targets=TARGET_HIGHLIGHT,\n",
    "        size=S,\n",
    "        bg_alpha=BG_ALPHA,\n",
    "        alpha=ALPHA,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "    )\n",
    "\n",
    "# distances = {\"absolute\": latents_distance(list(ae_latents.values()))}\n",
    "\n",
    "for col_i, (quant_mode, bin_size) in enumerate(col_config):\n",
    "    rel_attention = RelativeAttention(\n",
    "        n_anchors=NUM_ANCHORS,\n",
    "        n_classes=len(set(TARGETS)),\n",
    "        similarity_mode=RelativeEmbeddingMethod.INNER,\n",
    "        values_mode=ValuesMethod.SIMILARITIES,\n",
    "        normalization_mode=NormalizationMode.L2,\n",
    "    )\n",
    "    assert sum(x.numel() for x in rel_attention.parameters()) == 0\n",
    "    rels = []\n",
    "    for row_ax, (enc_name, latents), (a_enc_name, a_latents) in zip(\n",
    "        axes[1], ae_latents.items(), anchors_latents.items()\n",
    "    ):\n",
    "        assert enc_name == a_enc_name\n",
    "        rel = rel_attention(x=latents, anchors=a_latents)[AttentionOutput.SIMILARITIES]\n",
    "        rels.append(rel)\n",
    "        plot_latent_space(\n",
    "            row_ax,\n",
    "            to_df(rel, mode=REDUCTION),\n",
    "            targets=TARGET_HIGHLIGHT,\n",
    "            size=S,\n",
    "            bg_alpha=BG_ALPHA,\n",
    "            alpha=ALPHA,\n",
    "            cmap=cmap,\n",
    "            norm=norm,\n",
    "        )\n",
    "    # distances[f\"relative({quant_mode}, {bin_size})\"] = latents_distance(rels)\n",
    "\n",
    "# distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name: str = f\"vision-embeddings-qualitative-{REDUCTION}\"\n",
    "file_name_pdf: str = f\"{file_name}.pdf\"\n",
    "file_name_svg: str = f\"{file_name}.svg\"\n",
    "\n",
    "fig.savefig(f\"{file_name}.svg\", bbox_inches=\"tight\")\n",
    "!rsvg-convert -f pdf -o $file_name_pdf $file_name_svg\n",
    "!rm $file_name_svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHOR_CHOICES = (\"uniform\", \"fps\", \"kmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_IDS = [str(x) for x in range(len(data))][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rae.cka import CudaCKA as CKA\n",
    "\n",
    "EncPair = Tuple[str, str]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_retrieval(latent_space1: LatentSpace, latent_space2: LatentSpace, k: int = 5):\n",
    "    performance = {\n",
    "        key: []\n",
    "        for key in (\n",
    "            \"src_enc\",\n",
    "            \"tgt_enc\",\n",
    "            \"topk_jaccard\",\n",
    "            \"mrr\",\n",
    "            \"linear_cka\",\n",
    "            \"rbf_kernel_cka\",\n",
    "            \"mse\",\n",
    "            \"cosine_sim\",\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # index1: FaissIndex = latent_space1.to_faiss()\n",
    "    index2: FaissIndex = latent_space2.to_faiss()\n",
    "\n",
    "    target_neighbors = index2.search_by_vectors(\n",
    "        query_vectors=latent_space2.vectors.cpu().numpy(), k_most_similar=k, normalize=True\n",
    "    )\n",
    "    actual_neighbors = index2.search_by_vectors(\n",
    "        query_vectors=latent_space1.vectors.cpu().numpy(), k_most_similar=k, normalize=True\n",
    "    )\n",
    "\n",
    "    target_neighbors: Mapping[str, Mapping[str, float]] = {\n",
    "        word: topk for word, topk in zip(SEARCH_IDS, target_neighbors)\n",
    "    }\n",
    "    actual_neighbors: Mapping[str, Mapping[str, float]] = {\n",
    "        word: topk for word, topk in zip(SEARCH_IDS, actual_neighbors)\n",
    "    }\n",
    "\n",
    "    target_words: Mapping[str, Set[str]] = {\n",
    "        search_word: set(target_neighbors[search_word].keys()) for search_word in SEARCH_IDS\n",
    "    }\n",
    "    actual_words: Mapping[str, Set[str]] = {\n",
    "        search_word: set(actual_neighbors[search_word].keys()) for search_word in SEARCH_IDS\n",
    "    }\n",
    "\n",
    "    topk_jaccard = {\n",
    "        search_word: len(set.intersection(target_words[search_word], actual_words[search_word]))\n",
    "        / len(set.union(target_words[search_word], actual_words[search_word]))\n",
    "        for search_word in SEARCH_IDS\n",
    "    }\n",
    "    topk_jaccard = np.mean(list(topk_jaccard.values()))\n",
    "\n",
    "    search_word2word2rank = {\n",
    "        search_word: {key: index for index, key in enumerate(word2sim.keys(), start=1)}\n",
    "        for search_word, word2sim in actual_neighbors.items()\n",
    "    }\n",
    "    mrr = {\n",
    "        search_word: (\n",
    "            #                 word2rank.get(search_word, K)\n",
    "            0\n",
    "            if search_word not in word2rank\n",
    "            else 1 / word2rank[search_word]\n",
    "        )\n",
    "        for search_word, word2rank in search_word2word2rank.items()\n",
    "    }\n",
    "    mrr = np.mean(list(mrr.values()))\n",
    "\n",
    "    # semantic_horizon = []\n",
    "    # for search_word, neighbors in actual_words.items():\n",
    "    #     neighbor2ranking = {\n",
    "    #         neighbor: {\n",
    "    #             key: index\n",
    "    #             for index, key in enumerate(\n",
    "    #                 enc_type2enc_names2word2topk[\"absolute\"][(enc_name2, enc_name2)][neighbor].keys(), start=1\n",
    "    #             )\n",
    "    #         }\n",
    "    #         for neighbor in neighbors\n",
    "    #     }\n",
    "    #     neighbor2mrr = {\n",
    "    #         neighbor: (\n",
    "    #             #                 topk.get(search_word, K)\n",
    "    #             0\n",
    "    #             if search_word not in ranking\n",
    "    #             else 1 / ranking[search_word]\n",
    "    #         )\n",
    "    #         for neighbor, ranking in neighbor2ranking.items()\n",
    "    #     }\n",
    "    #     semantic_horizon.append(np.mean(list(neighbor2mrr.values())))\n",
    "    #\n",
    "    # semantic_horizon = np.mean(semantic_horizon)\n",
    "\n",
    "    chunk_size: int = 5000\n",
    "    num_chunks: int = (len(SEARCH_IDS) + chunk_size - 1) // chunk_size\n",
    "    linear_cka, rbf_kernel_cka, mse, cosine_sim = [], [], [], []\n",
    "    for chunk_latents1, chunk_latents2 in zip(\n",
    "        latent_space1.vectors.chunk(num_chunks), latent_space2.vectors.chunk(num_chunks)\n",
    "    ):\n",
    "        chunk_latents1 = chunk_latents1.cuda()\n",
    "        chunk_latents2 = chunk_latents2.cuda()\n",
    "        cka = CKA(device=DEVICE)\n",
    "\n",
    "        chunk_linear_cka = cka.linear_CKA(chunk_latents1, chunk_latents2).cpu()\n",
    "        # chunk_rbf_kernel_cka = cka.kernel_CKA(chunk_latents1, chunk_latents2).cpu()\n",
    "        chunk_cosine_sim = F.cosine_similarity(chunk_latents1, chunk_latents2).mean().cpu()\n",
    "        chunk_mse = F.mse_loss(chunk_latents1, chunk_latents2, reduction=\"sum\").cpu()\n",
    "\n",
    "        _ = chunk_latents1.cpu()\n",
    "        _ = chunk_latents2.cpu()\n",
    "\n",
    "        linear_cka.append(chunk_linear_cka)\n",
    "        rbf_kernel_cka.append(torch.zeros(1))\n",
    "        mse.append(chunk_mse)\n",
    "        cosine_sim.append(chunk_cosine_sim)\n",
    "\n",
    "    linear_cka = torch.stack(linear_cka).mean(dim=0).cpu().item()\n",
    "    rbf_kernel_cka = torch.stack(rbf_kernel_cka).mean(dim=0).cpu().item()\n",
    "    mse = torch.stack(mse).mean(dim=0).cpu().item()\n",
    "    cosine_sim = torch.stack(cosine_sim).mean(dim=0).cpu().item()\n",
    "\n",
    "    performance[\"src_enc\"].append(latent_space1.encoder)\n",
    "    performance[\"tgt_enc\"].append(latent_space2.encoder)\n",
    "    performance[\"topk_jaccard\"].append(topk_jaccard)\n",
    "    performance[\"mrr\"].append(mrr)\n",
    "    performance[\"linear_cka\"].append(linear_cka)\n",
    "    performance[\"rbf_kernel_cka\"].append(rbf_kernel_cka)\n",
    "    performance[\"mse\"].append(mse)\n",
    "    performance[\"cosine_sim\"].append(cosine_sim)\n",
    "\n",
    "    performance = pd.DataFrame(performance)\n",
    "    performance[\"enc1_type\"] = latent_space1.encoding_type\n",
    "    performance[\"enc2_type\"] = latent_space2.encoding_type\n",
    "\n",
    "    return performance\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(\n",
    "#     enc_type2enc_name2faiss_index: Mapping[str, Mapping[str, FaissIndex]],\n",
    "#     enc_type2enc_names2word2topk: Mapping[str, Mapping[Tuple[str, str], Mapping[str, Sequence[str]]]],\n",
    "# ):\n",
    "#     performance = {key: [] for key in (\"enc_name\", \"linear_cka\", \"rbf_kernel_cka\")}\n",
    "#\n",
    "#     for enc_name in (\"word2vec-google-news-300\", \"local_fasttext\"):\n",
    "#         faiss_abs = enc_type2enc_name2faiss_index[\"absolute\"][enc_name]\n",
    "#         faiss_rel = enc_type2enc_name2faiss_index[\"relative\"][enc_name]\n",
    "#\n",
    "#         linear_cka, rbf_kernel_cka = [], []\n",
    "#         for chunk in chunk_iterable(SEARCH_WORDS, chunk_size=5_000):\n",
    "#             chunk_latents_enc1 = torch.as_tensor(faiss_abs.reconstruct_n(keys=chunk), device=DEVICE)\n",
    "#             chunk_latents_enc2 = torch.as_tensor(faiss_rel.reconstruct_n(keys=chunk), device=DEVICE)\n",
    "#\n",
    "#             cka = CKA(device=DEVICE)\n",
    "#             linear_cka.append(cka.linear_CKA(chunk_latents_enc1, chunk_latents_enc2))\n",
    "#             rbf_kernel_cka.append(cka.kernel_CKA(chunk_latents_enc1, chunk_latents_enc2))\n",
    "#\n",
    "#         linear_cka = torch.stack(linear_cka).mean(dim=0).cpu().item()\n",
    "#         rbf_kernel_cka = torch.stack(rbf_kernel_cka).mean(dim=0).cpu().item()\n",
    "#\n",
    "#         performance[\"enc_name\"].append(enc_name)\n",
    "#         performance[\"linear_cka\"].append(linear_cka)\n",
    "#         performance[\"rbf_kernel_cka\"].append(rbf_kernel_cka)\n",
    "#\n",
    "#     return pd.DataFrame(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEEDS: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "\n",
    "performances = []\n",
    "anchor_infos = []\n",
    "for seed, anchor_choice in (pbar := tqdm(list(itertools.product(range(NUM_SEEDS), ANCHOR_CHOICES)))):\n",
    "    absolute_spaces: Sequence[LatentSpace] = [\n",
    "        LatentSpace(\n",
    "            encoding_type=\"absolute\",\n",
    "            vectors=data[encoder],\n",
    "            ids=SEARCH_IDS,\n",
    "            encoder=encoder,\n",
    "        )\n",
    "        for encoder in BENCHMARK_ENCODERS\n",
    "    ]\n",
    "\n",
    "    for absolute_space in absolute_spaces:\n",
    "        words: Sequence[str] = absolute_space.get_anchors(anchor_choice=anchor_choice, seed=seed)\n",
    "        anchors: torch.Tensor = get_latents(words=words, encoder=absolute_space.encoder)\n",
    "        anchor_info = {\n",
    "            \"seed\": seed,\n",
    "            \"anchor_choice\": anchor_choice,\n",
    "            \"anchors\": anchors,\n",
    "            \"words\": words,\n",
    "            \"encoder\": absolute_space.encoder,\n",
    "            \"dists\": pairwise_cosine_similarity(anchors, zero_diagonal=False),\n",
    "        }\n",
    "        anchor_infos.append(anchor_info)\n",
    "\n",
    "    for abs_space1, abs_space2 in itertools.product(absolute_spaces, repeat=2):\n",
    "        # absolute\n",
    "        absolute_performance = evaluate_retrieval(latent_space1=abs_space1, latent_space2=abs_space2)\n",
    "\n",
    "        absolute_performance[\"anchor_choice\"] = anchor_choice\n",
    "        absolute_performance[\"seed\"] = seed\n",
    "        performances.append(absolute_performance)\n",
    "\n",
    "        # relative\n",
    "        rel_space1: RelativeSpace = abs_space1.to_relative(seed=seed, anchor_choice=anchor_choice)\n",
    "        rel_space2: RelativeSpace = abs_space2.to_relative(anchors=rel_space1.anchors)\n",
    "        relative_performance = evaluate_retrieval(latent_space1=rel_space1, latent_space2=rel_space2)\n",
    "        relative_performance[\"anchor_choice\"] = anchor_choice\n",
    "        relative_performance[\"seed\"] = seed\n",
    "\n",
    "        performances.append(relative_performance)\n",
    "\n",
    "performances = pd.concat(performances)\n",
    "performances.to_csv(\n",
    "    PROJECT_ROOT / \"experiments\" / \"fig:latent-rotation-comparison\" / \"quantitative_analysis_cv.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    ")\n",
    "torch.save(anchor_infos, PROJECT_ROOT / \"experiments\" / \"fig:latent-rotation-comparison\" / \"anchor_infos_cv.pt\")\n",
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.read_csv(\n",
    "    PROJECT_ROOT / \"experiments\" / \"fig:latent-rotation-comparison\" / \"quantitative_analysis_cv.tsv\", sep=\"\\t\"\n",
    ")\n",
    "performance_df.groupby([\"anchor_choice\", \"enc1_type\", \"enc1_type\", \"src_enc\", \"tgt_enc\"]).aggregate(\n",
    "    [\n",
    "        np.mean,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
