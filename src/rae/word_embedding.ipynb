{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from torch import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rae.modules.attention import RelativeAttention\n",
    "from rae.modules.enumerations import (\n",
    "    NormalizationMode,\n",
    "    RelativeEmbeddingMethod,\n",
    "    ValuesMethod,\n",
    "    SimilaritiesQuantizationMode,\n",
    "    AttentionOutput,\n",
    ")\n",
    "from rae.openfaiss import FaissIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk = {\"English\": \"en\", \"Italian\": \"it\", \"Spanish\": \"es\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _read_synset_info() -> Mapping[str, Mapping[str, Any]]:\n",
    "    with (PROJECT_ROOT / \"data\" / \"synset2lang2lemmas.tsv\").open(\"r\", encoding=\"utf-8\") as fr:\n",
    "        head = next(fr).strip().split(\"\\t\")\n",
    "        langs = head[2:]\n",
    "        langs = [junk.get(lang, lang) for lang in langs]\n",
    "        synset2info = {}\n",
    "\n",
    "        lang2all_lemmas = {}\n",
    "\n",
    "        for i, line in enumerate(tqdm(fr, desc=\"Reading synset info\")):\n",
    "            # TODO: remove\n",
    "            #             if i > 20_000_000:\n",
    "            #                 break\n",
    "            synset_id, synset_type, *lemmas = line.strip(\"\\n\").split(\"\\t\")\n",
    "            if synset_type != \"CONCEPT\":\n",
    "                continue\n",
    "\n",
    "            assert len(langs) == len(lemmas)\n",
    "            lang2lemma = dict(zip(langs, lemmas))\n",
    "\n",
    "            lang2lemma = {lang: lang2lemma[lang] for lang in junk.values()}  # TODO: remove\n",
    "\n",
    "            if any(\"_\" in lemma for lang, lemma in lang2lemma.items()):\n",
    "                continue\n",
    "\n",
    "            if any(len(lemma) < 4 for lemma in lang2lemma.values()):\n",
    "                continue\n",
    "\n",
    "            if len(set(lang2lemma.values())) < len(lang2lemma.values()):\n",
    "                continue\n",
    "\n",
    "            for lang, lemma in lang2lemma.items():\n",
    "                lang2all_lemmas.setdefault(lang, set())\n",
    "                if lemma in lang2all_lemmas:\n",
    "                    continue\n",
    "                lang2all_lemmas[lang].add(lemma)\n",
    "\n",
    "            synset2info[synset_id] = dict(synset_type=synset_type, lang2lemma=lang2lemma, synset_id=synset_id)\n",
    "\n",
    "        return synset2info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_anchors(\n",
    "    lang2word2embedding: Mapping[str, Mapping[str, np.ndarray]], target_candidates: Optional[int] = 3_000\n",
    "):\n",
    "    synset_info = _read_synset_info()\n",
    "    candidates = []\n",
    "\n",
    "    for synset_id, info in tqdm(synset_info.items(), desc=\"Iterating synset info\"):\n",
    "        lang2lemma: Mapping[str, str] = info[\"lang2lemma\"]\n",
    "        if all(lang2word2embedding.get(lang, {}).get(lemma, None) is not None for lang, lemma in lang2lemma.items()):\n",
    "            candidates.append(info)\n",
    "\n",
    "        if target_candidates is not None and len(candidates) >= target_candidates:\n",
    "            break\n",
    "\n",
    "    lang2anchors = {}\n",
    "    for candidate in candidates:\n",
    "        for lang, lemma in candidate[\"lang2lemma\"].items():\n",
    "            lang2anchors.setdefault(lang, []).append(lemma)\n",
    "\n",
    "    return lang2anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings() -> Mapping[str, Mapping[str, np.ndarray]]:\n",
    "    def read_file(file_path: Path, max_index: Optional[int] = 10_000) -> Mapping[str, np.ndarray]:\n",
    "        with file_path.open(\"r\", encoding=\"utf-8\") as fr:\n",
    "            next(fr)\n",
    "\n",
    "            word2embedding = {}\n",
    "            for i, line in enumerate(tqdm(fr, desc=f\"Reading {file_path}\")):\n",
    "                if max_index is not None and i > max_index:\n",
    "                    break\n",
    "                word, *embedding = line.strip().split(\" \")\n",
    "                embedding = np.array([float(x) for x in embedding])\n",
    "                word2embedding[word] = embedding\n",
    "\n",
    "            return word2embedding\n",
    "\n",
    "    lang2word2embedding = {}\n",
    "\n",
    "    for file in (PROJECT_ROOT / \"fasttext\").iterdir():\n",
    "        if \".gz\" in file.suffixes:\n",
    "            continue\n",
    "        lang = file.suffixes[0].strip(\".\")\n",
    "        word2embedding = read_file(file_path=file, max_index=20_000)\n",
    "        lang2word2embedding[lang] = word2embedding\n",
    "\n",
    "    return lang2word2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2word2embedding = read_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lang2faiss_index = {lang: FaissIndex(d=300) for lang, word2embedding in lang2word2embedding.items()}\n",
    "for lang, faiss_index in abs_lang2faiss_index.items():\n",
    "    faiss_index: FaissIndex\n",
    "    faiss_index.add_vectors(embeddings=lang2word2embedding[lang].items(), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2anchors = build_anchors(lang2word2embedding=lang2word2embedding, target_candidates=1000)\n",
    "Path(PROJECT_ROOT / \"lang2anchors.json\").write_text(json.dumps(lang2anchors, indent=4))\n",
    "n_anchors: int = len(list(lang2anchors.values())[0])\n",
    "n_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_block: RelativeAttention = RelativeAttention(\n",
    "    in_features=300,\n",
    "    hidden_features=None,\n",
    "    n_anchors=n_anchors,\n",
    "    n_classes=None,\n",
    "    normalization_mode=NormalizationMode.L2,\n",
    "    similarity_mode=RelativeEmbeddingMethod.INNER,\n",
    "    values_mode=ValuesMethod.SIMILARITIES,\n",
    "    similarities_quantization_mode=SimilaritiesQuantizationMode.DIFFERENTIABLE_ROUND,\n",
    "    similarities_bin_size=0.5,\n",
    "    #     similarities_quantization_mode=None,\n",
    "    #     similarities_bin_size=None,\n",
    "    similarities_aggregation_mode=None,\n",
    "    similarities_aggregation_n_groups=None,\n",
    "    anchors_sampling_mode=None,\n",
    "    n_anchors_sampling_per_class=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_lang2faiss_index = {lang: FaissIndex(d=n_anchors) for lang, _ in lang2word2embedding.items()}\n",
    "lang2anchor_embeddings = {\n",
    "    lang: torch.stack([torch.tensor(lang2word2embedding[lang][anchor]).to(device) for anchor in anchors])\n",
    "    for lang, anchors in lang2anchors.items()\n",
    "}\n",
    "for lang, faiss_index in rel_lang2faiss_index.items():\n",
    "    word2embedding = lang2word2embedding[lang]\n",
    "    words, embeddings = list(zip(*word2embedding.items()))\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "    embeddings = attention_block(x=embeddings, anchors=lang2anchor_embeddings[lang])[AttentionOutput.OUTPUT]\n",
    "    faiss_index.add_vectors(embeddings=list(zip(words, embeddings.cpu().numpy())), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_en: str = \"shampoo\"\n",
    "word_it: str = \"shampoo\"\n",
    "word_en_rel_vector = rel_lang2faiss_index[\"en\"].reconstruct(word_en)\n",
    "word_en_abs_vector = abs_lang2faiss_index[\"en\"].reconstruct(word_en)\n",
    "word_it_rel_vector = rel_lang2faiss_index[\"it\"].reconstruct(word_it)\n",
    "word_it_abs_vector = abs_lang2faiss_index[\"it\"].reconstruct(word_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_lang2faiss_index[\"en\"].search_by_keys(query=[word_en], k_most_similar=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_lang2faiss_index[\"en\"].search_by_keys(query=[word_en], k_most_similar=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_lang2faiss_index[\"it\"].search_by_keys(query=[word_it], k_most_similar=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_lang2faiss_index[\"it\"].search_by_vectors(\n",
    "    query_vectors=np.array([word_en_rel_vector], dtype=\"float32\"),\n",
    "    k_most_similar=10,\n",
    "    normalize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_lang2faiss_index[\"en\"].search_by_vectors(\n",
    "    query_vectors=np.array([word_it_rel_vector], dtype=\"float32\"),\n",
    "    k_most_similar=10,\n",
    "    normalize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(\n",
    "    x1=torch.tensor(word_en_rel_vector),\n",
    "    x2=torch.tensor(word_it_rel_vector),\n",
    "    dim=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(\n",
    "    x1=torch.tensor(word_en_abs_vector),\n",
    "    x2=torch.tensor(word_it_abs_vector),\n",
    "    dim=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_diff = (torch.tensor(word_en_rel_vector) - torch.tensor(word_it_rel_vector)).abs().sum()\n",
    "rel_diff, rel_diff / n_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff = (torch.tensor(word_en_abs_vector) - torch.tensor(word_it_abs_vector)).abs().sum()\n",
    "abs_diff, abs_diff / 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
