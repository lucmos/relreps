data:
  _target_: rae.data.datamodule.MyDataModule

  anchors_mode: "fixed" # # "fixed", "random_images", "random_latents"
  anchors_num: 10

  # Different classes for anchors
  anchors_idxs: [0, 1, 2, 3, 4, 5, 7, 13, 15, 17]

#
#   ALl anchors from class seven
#  anchors_idx: [0, 17, 26, 34, 36, 41, 60, 64, 70, 75]

  latent_dim: ${nn.module.autoencoder.latent_dim}

  val_images_fixed_idxs: [7371, 3963, 2861, 1701, 3172,
                          1749, 7023, 1606, 6481, 1377,
                          6003, 3593, 3410, 3399, 7277,
                          5337, 968, 8206, 288, 1968,
                          5677, 9156, 8139, 7660, 7089,
                          1893, 3845, 2084, 1944, 3375,
                          4848, 8704, 6038, 2183, 7422,
                          2682, 6878, 6127, 2941, 5823,
                          9129, 1798, 6477, 9264, 476,
                          3007, 4992, 1428, 9901, 5388]

  datasets:
    train:
      _target_: rae.data.mnist.MNISTDataset

    val:
      - _target_: rae.data.mnist.MNISTDataset

#    test:
#      - _target_: rae.data.mnist.MNISTDataset

  gpus: ${train.trainer.gpus}

  num_workers:
    train: 8
    val: 4
    test: 0

  batch_size:
    train: 128
    val: 128
    test: 16


module:
  _target_: rae.pl_modules.pl_deterministic.LightningDeterministic

  plot_n_val_samples: 1000

  loss:
    variational_beta: 1

  autoencoder:
    _target_: rae.modules.ae.AE
    latent_dim: 2
    hidden_channels: 64
#    normalize_latents: False

  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.Adam
    #  These are all default parameters for the Adam optimizer
    lr: 1e-3
    betas: [ 0.9, 0.999 ]
    eps: 1e-08
    weight_decay: 0

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0 # min value for the lr
    last_epoch: -1
    verbose: False
