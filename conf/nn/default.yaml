data:
  _target_: rae.data.datamodule.MyDataModule

  anchors_mode: "stratified_subset" # "stratified", "stratified_subset", "fixed", "random_images", "random_latents"
  anchors_num: 10

  # Different classes for anchors
  anchors_idxs: null #[0, 1, 2, 3, 4, 5, 7, 13, 15, 17]

#
#   ALl anchors from class seven
#  anchors_idx: [0, 17, 26, 34, 36, 41, 60, 64, 70, 75]

  latent_dim: ${oc.select:nn.module.autoencoder.latent_dim,null}

  val_images_fixed_idxs: [7371, 3963, 2861, 1701, 3172,
                          1749, 7023, 1606, 6481, 1377,
                          6003, 3593, 3410, 3399, 7277,
                          5337, 968, 8206, 288, 1968,
                          5677, 9156, 8139, 7660, 7089,
                          1893, 3845, 2084, 1944, 3375,
                          4848, 8704, 6038, 2183, 7422,
                          2682, 6878, 6127, 2941, 5823,
                          9129, 1798, 6477, 9264, 476,
                          3007, 4992, 1428, 9901, 5388]

  datasets:
    train:
      _target_: rae.data.cifar100.CIFAR100Dataset

    val:
      - _target_: rae.data.cifar100.CIFAR100Dataset

#    test:
#      - _target_: rae.data.cifar100.CIFAR100Dataset

  transforms:
    _target_: torchvision.transforms.Compose
    transforms:
#      - _target_: torchvision.transforms.Normalize
      - _target_: torchvision.transforms.ToTensor

  gpus: ${train.trainer.gpus}

  num_workers:
    train: 8
    val: 4
    test: 0

  batch_size:
    train: 128
    val: 128
    test: 16

module:
  _target_: rae.pl_modules.pl_gclassifier.LightningClassifier

  plot_n_val_samples: 1000
  fit_pca_each_epoch: True

  loss:
    variational_beta: 1

  model:
    _target_: rae.modules.classifier.CNN
    hidden_channels: 64

#    reparametrize_anchors: False
#
#    normalize_latents: 'off'
#    normalize_only_anchors_latents: True
#
#    normalize_means: l2
#    normalize_only_anchors_means: False
#
#    relative_embedding_method: basis_change # inner, basis_change
#    normalize_relative_embedding: 'off'

  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.Adam
    #  These are all default parameters for the Adam optimizer
    lr: 1e-3
    betas: [ 0.9, 0.999 ]
    eps: 1e-08
    weight_decay: 0

#  lr_scheduler:
#    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#    T_0: 10
#    T_mult: 2
#    eta_min: 0 # min value for the lr
#    last_epoch: -1
#    verbose: False
