data:
  _target_: rae.data.datamodule.MyDataModule

  datasets:
    train:
      _target_: rae.data.mnist.MNISTDataset

    val:
      - _target_: rae.data.mnist.MNISTDataset

#    test:
#      - _target_: rae.data.mnist.MNISTDataset

  gpus: ${train.trainer.gpus}

  num_workers:
    train: 8
    val: 4
    test: 4

  batch_size:
    train: 128
    val: 128
    test: 16

  # example
  anchors_idxs: [0, 1, 2, 3, 4, 5, 7, 13, 15, 17]

module:
  _target_: rae.pl_modules.pl_rae.RAE

  plot_n_val_samples: 1000

  loss:
    variational_beta: 1

  autoencoder:
    _target_: rae.modules.vae.VAE
    latent_dim: 2
    hidden_channels: 64

  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.Adam
    #  These are all default parameters for the Adam optimizer
    lr: 1e-3
    betas: [ 0.9, 0.999 ]
    eps: 1e-08
    weight_decay: 0

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0 # min value for the lr
    last_epoch: -1
    verbose: False
