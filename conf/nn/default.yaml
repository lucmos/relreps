data:
  _target_: rae.data.datamodule.MyDataModule

  datasets:
    train:
      _target_: rae.data.mnist.MNISTDataset

#    val:
#      - _target_: rae.data.mnist.MNISTDataset

    test:
      - _target_: rae.data.mnist.MNISTDataset

  gpus: ${train.trainer.gpus}

  num_workers:
    train: 8
    val: 4
    test: 4

  batch_size:
    train: 32
    val: 16
    test: 16

  # example
  anchors_idxs: [0, 1, 2, 3, 4, 5, 7, 13, 15, 17]

module:
  _target_: rae.pl_modules.pl_module.MyLightningModule

  optimizer:
    #  Adam-oriented deep learning
    _target_: torch.optim.Adam
    #  These are all default parameters for the Adam optimizer
    lr: 0.001
    betas: [ 0.9, 0.999 ]
    eps: 1e-08
    weight_decay: 0

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0 # min value for the lr
    last_epoch: -1
    verbose: False
