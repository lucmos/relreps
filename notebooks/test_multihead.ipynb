{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718321d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rae.modules import attention\n",
    "from rae.modules.relative_classifier import ReprPooling\n",
    "from rae.modules.enumerations import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63315181",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = 512\n",
    "N_ANCHORS = 500\n",
    "N_CLASSES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_latents = torch.randn(8, IN_FEATURES, dtype=torch.double)\n",
    "batch_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_latents = torch.randn(N_ANCHORS, IN_FEATURES, dtype=torch.double)\n",
    "anchors_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718f6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5066c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbb225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25315787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadRelativeAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attentions: Sequence[attention.RelativeAttention],\n",
    "        hidden_features,\n",
    "        repr_pooling: ReprPooling = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_subspaces = len(attentions)\n",
    "        self.relative_attentions = attentions\n",
    "\n",
    "        self.subspace_features = hidden_features // self.num_subspaces\n",
    "        assert (hidden_features / self.num_subspaces) == (hidden_features // self.num_subspaces)\n",
    "\n",
    "        self.repr_pooling: ReprPooling = repr_pooling if repr_pooling is not None else ReprPooling.NONE\n",
    "\n",
    "        if self.repr_pooling not in set(ReprPooling):\n",
    "            raise ValueError(f\"Representation Pooling method not supported: {repr_pooling}\")\n",
    "\n",
    "        repr_dim: int = list(self.relative_attentions)[0].output_dim\n",
    "\n",
    "        if self.repr_pooling != ReprPooling.NONE:\n",
    "\n",
    "            self.classification_layer = nn.Linear(repr_dim, N_CLASSES)\n",
    "\n",
    "        else:\n",
    "            self.classification_layer = nn.Linear(sum(x.output_dim for x in self.relative_attentions), N_CLASSES)\n",
    "\n",
    "        if self.repr_pooling == ReprPooling.LINEAR:\n",
    "            self.head_pooling = nn.Linear(\n",
    "                in_features=sum(x.output_dim for x in self.relative_attentions),\n",
    "                out_features=repr_dim,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_latents: torch.Tensor,\n",
    "        anchors_latents: torch.Tensor,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        subspace_outputs = []\n",
    "        for i, relative_attention in enumerate(self.relative_attentions):\n",
    "            x_i_subspace = batch_latents[:, i * self.subspace_features : (i + 1) * self.subspace_features]\n",
    "            anchors_i_subspace = anchors_latents[:, i * self.subspace_features : (i + 1) * self.subspace_features]\n",
    "            subspace_output = relative_attention(\n",
    "                x=x_i_subspace,\n",
    "                anchors=anchors_i_subspace,\n",
    "            )\n",
    "            subspace_outputs.append(subspace_output)\n",
    "\n",
    "        attention_output = {key: [subspace[key] for subspace in subspace_outputs] for key in subspace_outputs[0].keys()}\n",
    "        for to_merge in (AttentionOutput.OUTPUT, AttentionOutput.SIMILARITIES):\n",
    "            attention_output[to_merge] = torch.stack(attention_output[to_merge], dim=1)\n",
    "\n",
    "        if self.repr_pooling == ReprPooling.LINEAR:\n",
    "            attention_output[AttentionOutput.OUTPUT] = torch.flatten(attention_output[AttentionOutput.OUTPUT], 1, 2)\n",
    "            attention_output[AttentionOutput.OUTPUT] = self.head_pooling(attention_output[AttentionOutput.OUTPUT])\n",
    "        elif self.repr_pooling == ReprPooling.MAX:\n",
    "            attention_output[AttentionOutput.OUTPUT] = attention_output[AttentionOutput.OUTPUT].max(dim=1)[0]\n",
    "        elif self.repr_pooling == ReprPooling.SUM:\n",
    "            attention_output[AttentionOutput.OUTPUT] = attention_output[AttentionOutput.OUTPUT].sum(dim=1)\n",
    "        elif self.repr_pooling == ReprPooling.MEAN:\n",
    "            attention_output[AttentionOutput.OUTPUT] = attention_output[AttentionOutput.OUTPUT].mean(dim=1)\n",
    "        elif self.repr_pooling == ReprPooling.NONE:\n",
    "            attention_output[AttentionOutput.OUTPUT] = torch.flatten(attention_output[AttentionOutput.OUTPUT], 1, 2)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return attention_output[AttentionOutput.OUTPUT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hidden_features\": 64,\n",
    "    \"transform_elements\": None,\n",
    "    #'dropout_p': 0.1,\n",
    "    \"normalization_mode\": \"off\",\n",
    "    \"similarity_mode\": \"inner\",\n",
    "    #'num_subspaces': 4,\n",
    "    \"values_mode\": \"similarities\",\n",
    "    \"values_self_attention_nhead\": 8,\n",
    "    \"similarities_quantization_mode\": None,\n",
    "    \"similarities_bin_size\": None,\n",
    "    \"similarities_aggregation_mode\": None,\n",
    "    \"similarities_aggregation_n_groups\": None,\n",
    "    \"anchors_sampling_mode\": None,\n",
    "    \"n_anchors_sampling_per_class\": None,\n",
    "    #'repr_pooling': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826574ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "relative_attention = attention.RelativeAttention(\n",
    "    in_features=IN_FEATURES, n_anchors=N_ANCHORS, n_classes=N_CLASSES, **params\n",
    ")\n",
    "relative_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "N_SUBSPACES = 8\n",
    "\n",
    "attentions = [\n",
    "    attention.RelativeAttention(in_features=IN_FEATURES, n_anchors=N_ANCHORS, n_classes=N_CLASSES, **params)\n",
    "    for _ in range(N_SUBSPACES)\n",
    "]\n",
    "multihead = MultiHeadRelativeAttention(attentions, hidden_features=IN_FEATURES, repr_pooling=\"sum\")\n",
    "multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_out = relative_attention(batch_latents, anchors_latents)[AttentionOutput.OUTPUT]\n",
    "mout = multihead(batch_latents, anchors_latents)\n",
    "torch.allclose(mout, rel_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cefc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a46f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a0d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc908cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7873d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e32032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47360fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f3438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530915e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f50c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
